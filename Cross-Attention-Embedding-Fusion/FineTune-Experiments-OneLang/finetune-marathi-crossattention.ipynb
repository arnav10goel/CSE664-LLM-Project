{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9926113,"sourceType":"datasetVersion","datasetId":6101008}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom transformers import BertTokenizerFast, BertModel\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport string\nimport json\n\nfrom transformers import AutoTokenizer, AutoModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T19:44:57.553914Z","iopub.execute_input":"2024-11-18T19:44:57.554490Z","iopub.status.idle":"2024-11-18T19:45:02.040990Z","shell.execute_reply.started":"2024-11-18T19:44:57.554455Z","shell.execute_reply":"2024-11-18T19:45:02.040297Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"lang = 'marathi'\n\n# Load train data from json\nwith open(f'/kaggle/input/llm-project/{lang}_train.json') as f:\n    train_data = json.load(f)\n\n# Load test data from json\nwith open(f'/kaggle/input/llm-project/{lang}_test.json') as f:\n    test_data = json.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T19:45:02.042546Z","iopub.execute_input":"2024-11-18T19:45:02.043034Z","iopub.status.idle":"2024-11-18T19:45:02.098609Z","shell.execute_reply.started":"2024-11-18T19:45:02.042996Z","shell.execute_reply":"2024-11-18T19:45:02.097923Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load tokeniser for MBERT\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T19:45:02.099599Z","iopub.execute_input":"2024-11-18T19:45:02.099968Z","iopub.status.idle":"2024-11-18T19:45:03.683221Z","shell.execute_reply.started":"2024-11-18T19:45:02.099922Z","shell.execute_reply":"2024-11-18T19:45:03.682256Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e2c1cf77c7b48eb9690c9908d54ac67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7788e6608b7f4f8b8f2a779e2307e1cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"455824d10d5641a0b8573db66ae5d80e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c805a82d9c6455ea53cbc2579a8e80d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"class NER_Dataset(Dataset):\n    def __init__(self, data, tokenizer, max_len=128):\n        self.data = data  # List of sentences and labels\n        self.tokenizer = tokenizer  # mBERT tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text = self.data[idx][\"tokens\"]\n        word_labels = self.data[idx][\"ner_tags\"]\n\n        # Tokenize the text and align the labels\n        encoding = self.tokenizer(text,\n                                  is_split_into_words=True,\n                                  return_offsets_mapping=True,\n                                  padding='max_length',\n                                  truncation=True,\n                                  max_length=self.max_len)\n\n        labels = [self.align_labels(encoding['offset_mapping'], word_labels)]\n\n        # Remove the offset mapping to prevent issues during model training\n        del encoding['offset_mapping']\n\n        item = {key: torch.tensor(val) for key, val in encoding.items()}\n        item['labels'] = torch.tensor(labels[0], dtype=torch.long)\n\n        return item\n    \n    # Create a function to align labels\n    def align_labels(self, offset_mapping, labels):\n        aligned_labels = []\n        current_label_index = 0\n\n        for offset in offset_mapping:\n            # If the offset mapping is (0, 0), it's a special token ([CLS], [SEP], [PAD])\n            if offset == (0, 0):\n                aligned_labels.append(-100)  # -100 is used to ignore these tokens in the loss computation\n            else:\n                # Check if the token is the start of a new word\n                if offset[0] == 0:\n                    aligned_labels.append(labels[current_label_index])\n                    current_label_index += 1\n                else:\n                    # If the token is not the first subtoken, you can decide how to label it. \n                    # For simplicity, let's use the same label as the first subtoken\n                    aligned_labels.append(labels[current_label_index - 1])\n\n        return aligned_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T19:45:03.685043Z","iopub.execute_input":"2024-11-18T19:45:03.685341Z","iopub.status.idle":"2024-11-18T19:45:03.693462Z","shell.execute_reply.started":"2024-11-18T19:45:03.685314Z","shell.execute_reply":"2024-11-18T19:45:03.692585Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Create train dataset and test dataset\ntrain_dataset = NER_Dataset(train_data, tokenizer)\ntest_dataset = NER_Dataset(test_data, tokenizer)\n\n# Create train dataloader and test dataloader\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T19:45:03.694696Z","iopub.execute_input":"2024-11-18T19:45:03.695513Z","iopub.status.idle":"2024-11-18T19:45:03.709169Z","shell.execute_reply.started":"2024-11-18T19:45:03.695469Z","shell.execute_reply":"2024-11-18T19:45:03.708450Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nDEVICE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T19:45:03.710148Z","iopub.execute_input":"2024-11-18T19:45:03.710407Z","iopub.status.idle":"2024-11-18T19:45:03.779154Z","shell.execute_reply.started":"2024-11-18T19:45:03.710368Z","shell.execute_reply":"2024-11-18T19:45:03.778228Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass CrossAttentionFusion(nn.Module):\n    def __init__(self, hidden_dim, num_heads):\n        super(CrossAttentionFusion, self).__init__()\n        self.cross_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads)\n        self.layer_norm = nn.LayerNorm(hidden_dim)\n\n    def forward(self, embeddings_mbert, embeddings_indicbert):\n        \"\"\"\n        Fuse embeddings from mBERT and Indic-BERT using cross-attention.\n\n        Args:\n            embeddings_mbert (torch.Tensor): Embeddings from mBERT of shape [batch_size, seq_len_m, hidden_dim].\n            embeddings_indicbert (torch.Tensor): Embeddings from Indic-BERT of shape [batch_size, seq_len_i, hidden_dim].\n\n        Returns:\n            torch.Tensor: Fused embeddings of shape [batch_size, seq_len_m, hidden_dim].\n        \"\"\"\n        embeddings_mbert = embeddings_mbert.to(DEVICE)\n        embeddings_indicbert = embeddings_indicbert.to(DEVICE)\n        \n        # Transpose to shape [seq_len, batch_size, hidden_dim] for nn.MultiheadAttention\n        embeddings_mbert = embeddings_mbert.transpose(0, 1)\n        embeddings_indicbert = embeddings_indicbert.transpose(0, 1)\n\n        # Cross-attention: Indic BERT queries attend to m-BERT keys and values\n        attn_output, _ = self.cross_attention(\n            query=embeddings_indicbert,       # Queries from Indic-BERT\n            key=embeddings_mbert,     # Keys from m-BERT\n            value=embeddings_mbert    # Values from m-BERT\n        )\n\n        attn_output.to(DEVICE)\n\n        # Residual connection and layer normalization\n        output = self.layer_norm(attn_output + embeddings_mbert)\n\n        # Transpose back to [batch_size, seq_len_m, hidden_dim]\n        output = output.transpose(0, 1)\n\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T19:45:03.828393Z","iopub.execute_input":"2024-11-18T19:45:03.828642Z","iopub.status.idle":"2024-11-18T19:45:03.835233Z","shell.execute_reply.started":"2024-11-18T19:45:03.828616Z","shell.execute_reply":"2024-11-18T19:45:03.834348Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"hidden_dim = 768\nnum_heads = 4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T19:45:03.885006Z","iopub.execute_input":"2024-11-18T19:45:03.885222Z","iopub.status.idle":"2024-11-18T19:45:03.889138Z","shell.execute_reply.started":"2024-11-18T19:45:03.885200Z","shell.execute_reply":"2024-11-18T19:45:03.888295Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Model for NER\n\nclass MBERT_NER(nn.Module):\n    def __init__(self, num_labels, gru_hidden_size, num_gru_layers, freeze_bert=False):\n        super(MBERT_NER, self).__init__()\n\n        self.bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n        self.indic_bert = AutoModel.from_pretrained(\"ai4bharat/indic-bert\")\n        \n        # Initialize the Fusion Attention\n        self.attention = CrossAttentionFusion(hidden_dim=hidden_dim, num_heads=num_heads).to(DEVICE)\n        \n        self.gru = nn.GRU(input_size=self.bert.config.hidden_size * 3,\n                          hidden_size=gru_hidden_size,\n                          num_layers=num_gru_layers,\n                          batch_first=True)\n        self.classifier = nn.Linear(gru_hidden_size, num_labels)\n        self.dropout = nn.Dropout(0.1)\n        self.batch_norm = nn.BatchNorm1d(gru_hidden_size)\n\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n            for param in self.indic_bert.parameters():\n                param.requires_grad = False\n\n    def forward(self, input_ids, attention_mask):\n        \n        input_ids = input_ids.to(DEVICE)\n        attention_mask = attention_mask.to(DEVICE)\n        \n        # Embeddings\n        outputs1 = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        outputs2 = self.indic_bert(input_ids = input_ids, attention_mask = attention_mask)        \n        \n        embeddings_mbert = outputs1.last_hidden_state.to(DEVICE)\n        embeddings_indicbert = outputs2.last_hidden_state.to(DEVICE)\n        \n        # Fused embeddings\n        embeddings = self.attention(embeddings_mbert, embeddings_indicbert).to(DEVICE)\n        \n        sequence_output = torch.cat((embeddings_mbert, embeddings_indicbert, embeddings), dim=-1)\n        \n        gru_output, _ = self.gru(sequence_output)\n        gru_output = self.batch_norm(gru_output)\n        gru_output = self.dropout(gru_output)\n        \n        logits = self.classifier(gru_output)\n        return logits\n\n# Create the NER model\nNUM_LABELS = 7 # Number of NER tags\nGRU_HIDDEN_SIZE = 128 # Hidden size of the GRU\nNUM_GRU_LAYERS = 1 # Number of layers in the GRU\nFREEZE_BERT = False # Whether to freeze the BERT model\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmodel = MBERT_NER(num_labels=NUM_LABELS,\n                    gru_hidden_size=GRU_HIDDEN_SIZE,\n                    num_gru_layers=NUM_GRU_LAYERS,\n                    freeze_bert=FREEZE_BERT)\n\nmodel.to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T19:45:06.768864Z","iopub.execute_input":"2024-11-18T19:45:06.769269Z","iopub.status.idle":"2024-11-18T19:45:13.627800Z","shell.execute_reply.started":"2024-11-18T19:45:06.769235Z","shell.execute_reply":"2024-11-18T19:45:13.626934Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9eb657eb1165464fb6b4161b7b783bf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e65b457378b42149a00bf6016c5bdb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e20de502f70d4e40b60ba618c63db2bf"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"MBERT_NER(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (indic_bert): AlbertModel(\n    (embeddings): AlbertEmbeddings(\n      (word_embeddings): Embedding(200000, 128, padding_idx=0)\n      (position_embeddings): Embedding(512, 128)\n      (token_type_embeddings): Embedding(2, 128)\n      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0, inplace=False)\n    )\n    (encoder): AlbertTransformer(\n      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n      (albert_layer_groups): ModuleList(\n        (0): AlbertLayerGroup(\n          (albert_layers): ModuleList(\n            (0): AlbertLayer(\n              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (attention): AlbertSdpaAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (attention_dropout): Dropout(p=0, inplace=False)\n                (output_dropout): Dropout(p=0, inplace=False)\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              )\n              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n              (activation): GELUActivation()\n              (dropout): Dropout(p=0, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (pooler): Linear(in_features=768, out_features=768, bias=True)\n    (pooler_activation): Tanh()\n  )\n  (attention): CrossAttentionFusion(\n    (cross_attention): MultiheadAttention(\n      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n    )\n    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (gru): GRU(2304, 128, batch_first=True)\n  (classifier): Linear(in_features=128, out_features=7, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T19:45:16.169649Z","iopub.execute_input":"2024-11-18T19:45:16.169993Z","iopub.status.idle":"2024-11-18T19:45:16.173834Z","shell.execute_reply.started":"2024-11-18T19:45:16.169965Z","shell.execute_reply":"2024-11-18T19:45:16.173007Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from sklearn.metrics import f1_score, accuracy_score\n\n# Optimizer and loss function\noptimizer = optim.AdamW(model.parameters(), lr=2e-5)\ncriterion = nn.CrossEntropyLoss(ignore_index=-100)\n\n# Training loop\nEPOCHS = 6\n\ntrain_losses = []\ntrain_f1_scores = []\ntrain_acc_scores = []\nval_losses = []\nval_f1_scores = []\nval_acc_scores = []\n\n\nfor epoch in range(EPOCHS):\n    model.train()\n    train_loss = 0\n    train_predictions, train_labels = [], []\n\n    for batch in train_dataloader:\n        input_ids = batch['input_ids'].to(DEVICE)\n        attention_mask = batch['attention_mask'].to(DEVICE)\n        labels = batch['labels'].to(DEVICE)\n\n        optimizer.zero_grad()\n        logits = model(input_ids, attention_mask)\n        logits = logits.view(-1, NUM_LABELS)\n        labels = labels.view(-1)\n\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n        # Get predictions and filter out ignored indices for metric calculations\n        predictions = torch.argmax(logits, dim=-1)\n        active_indices = labels != -100\n        train_predictions.extend(predictions[active_indices].cpu().numpy())\n        train_labels.extend(labels[active_indices].cpu().numpy())\n\n    train_loss /= len(train_dataloader)\n    train_losses.append(train_loss)\n\n    train_f1 = f1_score(train_labels, train_predictions, average='macro')\n    train_f1_scores.append(train_f1)\n\n    train_acc = accuracy_score(train_labels, train_predictions)\n    train_acc_scores.append(train_acc)\n\n    # Validation loop\n    model.eval()\n    val_loss = 0\n    val_predictions, val_labels = [], []\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            input_ids = batch['input_ids'].to(DEVICE)\n            attention_mask = batch['attention_mask'].to(DEVICE)\n            labels = batch['labels'].to(DEVICE)\n\n            logits = model(input_ids, attention_mask)\n            logits = logits.view(-1, NUM_LABELS)\n            labels = labels.view(-1)\n\n            loss = criterion(logits, labels)\n            val_loss += loss.item()\n\n            # Filter predictions and labels\n            predictions = torch.argmax(logits, dim=-1)\n            active_indices = labels != -100\n            val_predictions.extend(predictions[active_indices].cpu().numpy())\n            val_labels.extend(labels[active_indices].cpu().numpy())\n\n    val_loss /= len(test_dataloader)\n    val_losses.append(val_loss)\n\n    val_f1 = f1_score(val_labels, val_predictions, average='macro')\n    val_f1_scores.append(val_f1)\n\n    val_acc = accuracy_score(val_labels, val_predictions)\n    val_acc_scores.append(val_acc)\n\n    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}, Val Acc: {val_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T19:45:16.987312Z","iopub.execute_input":"2024-11-18T19:45:16.987665Z","iopub.status.idle":"2024-11-18T20:03:07.146249Z","shell.execute_reply.started":"2024-11-18T19:45:16.987629Z","shell.execute_reply":"2024-11-18T20:03:07.145301Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/6, Train Loss: 0.6493, Train F1: 0.6739, Train Acc: 0.7982, Val Loss: 0.3572, Val F1: 0.8260, Val Acc: 0.8966\nEpoch 2/6, Train Loss: 0.2823, Train F1: 0.8647, Train Acc: 0.9185, Val Loss: 0.2945, Val F1: 0.8586, Val Acc: 0.9147\nEpoch 3/6, Train Loss: 0.1953, Train F1: 0.9080, Train Acc: 0.9444, Val Loss: 0.2891, Val F1: 0.8731, Val Acc: 0.9203\nEpoch 4/6, Train Loss: 0.1360, Train F1: 0.9370, Train Acc: 0.9627, Val Loss: 0.2748, Val F1: 0.8892, Val Acc: 0.9315\nEpoch 5/6, Train Loss: 0.1042, Train F1: 0.9511, Train Acc: 0.9716, Val Loss: 0.2541, Val F1: 0.8992, Val Acc: 0.9375\nEpoch 6/6, Train Loss: 0.0795, Train F1: 0.9651, Train Acc: 0.9796, Val Loss: 0.2679, Val F1: 0.8993, Val Acc: 0.9360\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# Load the test data from json for all 5 languages\nlanguages = ['hindi', 'bengali', 'marathi', 'tamil', 'telugu']\n\n# Iterate over all languages and evaluate the model\nfor lang in languages:\n    with open(f'/kaggle/input/llm-project/{lang}_test.json') as f:\n        test_data = json.load(f)\n\n    test_dataset = NER_Dataset(test_data, tokenizer)\n    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n    model.eval()\n    test_predictions, test_labels = [], []\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            input_ids = batch['input_ids'].to(DEVICE)\n            attention_mask = batch['attention_mask'].to(DEVICE)\n            labels = batch['labels'].to(DEVICE)\n\n            logits = model(input_ids, attention_mask)\n            logits = logits.view(-1, NUM_LABELS)\n            labels = labels.view(-1)\n\n            # Filter predictions and labels\n            predictions = torch.argmax(logits, dim=-1)\n            active_indices = labels != -100\n            test_predictions.extend(predictions[active_indices].cpu().numpy())\n            test_labels.extend(labels[active_indices].cpu().numpy())\n\n    weighted_f1 = f1_score(test_labels, test_predictions, average='weighted')\n    macro_f1 = f1_score(test_labels, test_predictions, average='macro')\n    accuracy = accuracy_score(test_labels, test_predictions)\n\n    if lang == 'hindi':\n        LANG = \"Hindi\"\n    elif lang == 'bengali':\n        LANG = \"Bengali\"\n    elif lang == 'marathi':\n        LANG = \"Marathi\"\n    elif lang == 'tamil':\n        LANG = \"Tamil\"\n    elif lang == 'telugu':\n        LANG = \"Telugu\"\n\n    print(f\"Language: {LANG}\")\n    print()\n    print(f\"Weighted F1: {weighted_f1:.4f}\")\n    print(f\"Macro F1: {macro_f1:.4f}\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print()\n    print(\"CLASSIFICATION REPORT: \")\n    print(classification_report(test_labels, test_predictions))\n    print()\n    print(\"CONFUSION MATRIX: \")\n    print(confusion_matrix(test_labels, test_predictions))\n    print(\"--------------------------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T20:03:07.147932Z","iopub.execute_input":"2024-11-18T20:03:07.148423Z","iopub.status.idle":"2024-11-18T20:04:15.040841Z","shell.execute_reply.started":"2024-11-18T20:03:07.148370Z","shell.execute_reply":"2024-11-18T20:04:15.039951Z"}},"outputs":[{"name":"stdout","text":"Language: Hindi\n\nWeighted F1: 0.8041\nMacro F1: 0.7585\nAccuracy: 0.8032\n\nCLASSIFICATION REPORT: \n              precision    recall  f1-score   support\n\n           0       0.92      0.82      0.87      5277\n           1       0.75      0.84      0.79      1481\n           2       0.79      0.91      0.85      1874\n           3       0.61      0.67      0.64       986\n           4       0.80      0.82      0.81      2602\n           5       0.69      0.71      0.70      1195\n           6       0.69      0.61      0.65       867\n\n    accuracy                           0.80     14282\n   macro avg       0.75      0.77      0.76     14282\nweighted avg       0.81      0.80      0.80     14282\n\n\nCONFUSION MATRIX: \n[[4349  252   96  238   92  201   49]\n [  26 1239  146   29   11   30    0]\n [  28   31 1710    6   80    0   19]\n [  63   82    4  659   90   85    3]\n [ 172   10  140   20 2129   30  101]\n [  37   34   17  117   73  854   63]\n [  59    0   43    3  190   41  531]]\n--------------------------------------------------\nLanguage: Bengali\n\nWeighted F1: 0.7852\nMacro F1: 0.7852\nAccuracy: 0.7878\n\nCLASSIFICATION REPORT: \n              precision    recall  f1-score   support\n\n           0       0.81      0.90      0.85      1835\n           1       0.71      0.87      0.78      1343\n           2       0.76      0.84      0.80      2490\n           3       0.85      0.78      0.81      1239\n           4       0.81      0.80      0.80      3068\n           5       0.78      0.74      0.76      1510\n           6       0.82      0.59      0.69      2080\n\n    accuracy                           0.79     13565\n   macro avg       0.79      0.79      0.79     13565\nweighted avg       0.79      0.79      0.79     13565\n\n\nCONFUSION MATRIX: \n[[1660   30   13   22   84   19    7]\n [  37 1165   25   25    4   80    7]\n [ 156   33 2082    0  114   14   91]\n [   8  161    0  971   12   87    0]\n [ 107   17  286   15 2457   22  164]\n [  22  227    0  113   24 1117    7]\n [  71    2  332    0  347   93 1235]]\n--------------------------------------------------\nLanguage: Marathi\n\nWeighted F1: 0.9360\nMacro F1: 0.8993\nAccuracy: 0.9360\n\nCLASSIFICATION REPORT: \n              precision    recall  f1-score   support\n\n           0       0.97      0.96      0.97     10408\n           1       0.90      0.95      0.93      1342\n           2       0.95      0.96      0.95      1992\n           3       0.86      0.85      0.85      1172\n           4       0.91      0.96      0.93      2535\n           5       0.89      0.88      0.88      1829\n           6       0.79      0.78      0.78       820\n\n    accuracy                           0.94     20098\n   macro avg       0.90      0.90      0.90     20098\nweighted avg       0.94      0.94      0.94     20098\n\n\nCONFUSION MATRIX: \n[[9971   64   38   64   84  119   68]\n [  31 1280    7    8    0   16    0]\n [  29   14 1909    0   21    0   19]\n [  62   51    0  991   13   50    5]\n [  25    0   48    4 2421    0   37]\n [  85   16    0   87    0 1603   38]\n [  34    0    9    0  119   21  637]]\n--------------------------------------------------\nLanguage: Tamil\n\nWeighted F1: 0.8126\nMacro F1: 0.7026\nAccuracy: 0.8103\n\nCLASSIFICATION REPORT: \n              precision    recall  f1-score   support\n\n           0       0.94      0.89      0.92     11461\n           1       0.74      0.74      0.74      1643\n           2       0.85      0.76      0.80      2295\n           3       0.56      0.45      0.50      1512\n           4       0.68      0.80      0.73      2532\n           5       0.67      0.82      0.74      1881\n           6       0.42      0.58      0.49       677\n\n    accuracy                           0.81     22001\n   macro avg       0.70      0.72      0.70     22001\nweighted avg       0.82      0.81      0.81     22001\n\n\nCONFUSION MATRIX: \n[[10242   187    83   339   314   209    87]\n [   58  1213   127    72    23   128    22]\n [  167   124  1733    15    92    22   142]\n [  121    80     3   683   283   316    26]\n [  174     7    76    15  2023    47   190]\n [  106    22     3    90    54  1543    63]\n [   50     0    14     1   195    27   390]]\n--------------------------------------------------\nLanguage: Telugu\n\nWeighted F1: 0.8330\nMacro F1: 0.6799\nAccuracy: 0.8260\n\nCLASSIFICATION REPORT: \n              precision    recall  f1-score   support\n\n           0       0.97      0.87      0.92     17291\n           1       0.55      0.71      0.62      1810\n           2       0.70      0.87      0.78      1353\n           3       0.41      0.41      0.41      1375\n           4       0.69      0.79      0.74      2269\n           5       0.73      0.86      0.79      2072\n           6       0.44      0.57      0.50       455\n\n    accuracy                           0.83     26625\n   macro avg       0.64      0.73      0.68     26625\nweighted avg       0.85      0.83      0.83     26625\n\n\nCONFUSION MATRIX: \n[[15124   629   128   469   445   333   163]\n [  125  1285   217   113    22    46     2]\n [   25    31  1181     0   107     0     9]\n [  123   361    18   569   134   146    24]\n [   96     0   103   149  1792    57    72]\n [   76    35    11    90    21  1782    57]\n [   52     0    19     0    60    64   260]]\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Save the model\ntorch.save(model.state_dict(), 'NER_FineTune_Ma_cross_attn.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T20:08:16.228333Z","iopub.execute_input":"2024-11-18T20:08:16.228708Z","iopub.status.idle":"2024-11-18T20:08:17.600889Z","shell.execute_reply.started":"2024-11-18T20:08:16.228674Z","shell.execute_reply":"2024-11-18T20:08:17.600112Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}