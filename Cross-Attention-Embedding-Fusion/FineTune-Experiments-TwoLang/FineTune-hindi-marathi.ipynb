{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38a7a0cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:08:21.423521Z",
     "iopub.status.busy": "2024-11-18T20:08:21.423353Z",
     "iopub.status.idle": "2024-11-18T20:08:21.428091Z",
     "shell.execute_reply": "2024-11-18T20:08:21.427746Z"
    },
    "papermill": {
     "duration": 0.011314,
     "end_time": "2024-11-18T20:08:21.429085",
     "exception": false,
     "start_time": "2024-11-18T20:08:21.417771",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "lang1 = \"hindi\"\n",
    "lang2 = \"bengali\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c6c24b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:08:21.435415Z",
     "iopub.status.busy": "2024-11-18T20:08:21.435292Z",
     "iopub.status.idle": "2024-11-18T20:08:21.437481Z",
     "shell.execute_reply": "2024-11-18T20:08:21.437145Z"
    },
    "papermill": {
     "duration": 0.005902,
     "end_time": "2024-11-18T20:08:21.438314",
     "exception": false,
     "start_time": "2024-11-18T20:08:21.432412",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "lang1 = \"hindi\"\n",
    "lang2 = \"marathi\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "273bcd3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:08:21.443129Z",
     "iopub.status.busy": "2024-11-18T20:08:21.443014Z",
     "iopub.status.idle": "2024-11-18T20:08:21.445446Z",
     "shell.execute_reply": "2024-11-18T20:08:21.445090Z"
    },
    "papermill": {
     "duration": 0.005792,
     "end_time": "2024-11-18T20:08:21.446221",
     "exception": false,
     "start_time": "2024-11-18T20:08:21.440429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with lang1: hindi, lang2: marathi\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running with lang1: {lang1}, lang2: {lang2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d033424",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:08:21.451262Z",
     "iopub.status.busy": "2024-11-18T20:08:21.451112Z",
     "iopub.status.idle": "2024-11-18T20:08:25.111329Z",
     "shell.execute_reply": "2024-11-18T20:08:25.110701Z"
    },
    "papermill": {
     "duration": 3.664677,
     "end_time": "2024-11-18T20:08:25.113127",
     "exception": false,
     "start_time": "2024-11-18T20:08:21.448450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 20:08:23.525840: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-18 20:08:23.525868: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-18 20:08:23.527226: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-18 20:08:23.533271: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 20:08:24.294451: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import string\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f097ab6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:08:25.125911Z",
     "iopub.status.busy": "2024-11-18T20:08:25.125455Z",
     "iopub.status.idle": "2024-11-18T20:08:25.153016Z",
     "shell.execute_reply": "2024-11-18T20:08:25.152463Z"
    },
    "papermill": {
     "duration": 0.035613,
     "end_time": "2024-11-18T20:08:25.154638",
     "exception": false,
     "start_time": "2024-11-18T20:08:25.119025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load train data from json (L1)\n",
    "with open(f\"/home/ashu/Desktop/sem7/llm/CSE664-LLM-Project/Cross-Attention-Embedding-Fusion/Dataset/{lang1}_train.json\") as f:\n",
    "    train_data_1 = json.load(f)\n",
    "\n",
    "# Load test data from json\n",
    "with open(f\"/home/ashu/Desktop/sem7/llm/CSE664-LLM-Project/Cross-Attention-Embedding-Fusion/Dataset/{lang1}_test.json\") as f:\n",
    "    test_data_1 = json.load(f)\n",
    "\n",
    "# Load train data from json (L2)\n",
    "with open(f\"/home/ashu/Desktop/sem7/llm/CSE664-LLM-Project/Cross-Attention-Embedding-Fusion/Dataset/{lang2}_train.json\") as f:\n",
    "    train_data_2 = json.load(f)\n",
    "\n",
    "# Load test data from json\n",
    "with open(f\"/home/ashu/Desktop/sem7/llm/CSE664-LLM-Project/Cross-Attention-Embedding-Fusion/Dataset/{lang2}_test.json\") as f:\n",
    "    test_data_2 = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2864575c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:08:25.166173Z",
     "iopub.status.busy": "2024-11-18T20:08:25.165967Z",
     "iopub.status.idle": "2024-11-18T20:08:25.168913Z",
     "shell.execute_reply": "2024-11-18T20:08:25.168405Z"
    },
    "papermill": {
     "duration": 0.010635,
     "end_time": "2024-11-18T20:08:25.170647",
     "exception": false,
     "start_time": "2024-11-18T20:08:25.160012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with lang1: hindi, lang2: marathi\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running with lang1: {lang1}, lang2: {lang2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caff37a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:08:25.182733Z",
     "iopub.status.busy": "2024-11-18T20:08:25.182361Z",
     "iopub.status.idle": "2024-11-18T20:08:25.184837Z",
     "shell.execute_reply": "2024-11-18T20:08:25.184355Z"
    },
    "papermill": {
     "duration": 0.010211,
     "end_time": "2024-11-18T20:08:25.186395",
     "exception": false,
     "start_time": "2024-11-18T20:08:25.176184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge the two datasets\n",
    "train_data = train_data_1 + train_data_2\n",
    "test_data = test_data_1 + test_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d359e8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:08:25.198221Z",
     "iopub.status.busy": "2024-11-18T20:08:25.197979Z",
     "iopub.status.idle": "2024-11-18T20:08:25.665080Z",
     "shell.execute_reply": "2024-11-18T20:08:25.664317Z"
    },
    "papermill": {
     "duration": 0.474231,
     "end_time": "2024-11-18T20:08:25.666144",
     "exception": false,
     "start_time": "2024-11-18T20:08:25.191913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load tokeniser\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09cc4f4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:08:25.672963Z",
     "iopub.status.busy": "2024-11-18T20:08:25.672699Z",
     "iopub.status.idle": "2024-11-18T20:08:25.677780Z",
     "shell.execute_reply": "2024-11-18T20:08:25.677285Z"
    },
    "papermill": {
     "duration": 0.010499,
     "end_time": "2024-11-18T20:08:25.679221",
     "exception": false,
     "start_time": "2024-11-18T20:08:25.668722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NER_Dataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=128):\n",
    "        self.data = data  # List of sentences and labels\n",
    "        self.tokenizer = tokenizer  # mBERT tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx][\"tokens\"]\n",
    "        word_labels = self.data[idx][\"ner_tags\"]\n",
    "\n",
    "        # Tokenize the text and align the labels\n",
    "        encoding = self.tokenizer(text,\n",
    "                                  is_split_into_words=True,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=self.max_len)\n",
    "\n",
    "        labels = [self.align_labels(encoding['offset_mapping'], word_labels)]\n",
    "\n",
    "        # Remove the offset mapping to prevent issues during model training\n",
    "        del encoding['offset_mapping']\n",
    "\n",
    "        item = {key: torch.tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels[0], dtype=torch.long)\n",
    "\n",
    "        return item\n",
    "    \n",
    "    # Create a function to align labels\n",
    "    def align_labels(self, offset_mapping, labels):\n",
    "        aligned_labels = []\n",
    "        current_label_index = 0\n",
    "\n",
    "        for offset in offset_mapping:\n",
    "            # If the offset mapping is (0, 0), it's a special token ([CLS], [SEP], [PAD])\n",
    "            if offset == (0, 0):\n",
    "                aligned_labels.append(-100)  # -100 is used to ignore these tokens in the loss computation\n",
    "            else:\n",
    "                # Check if the token is the start of a new word\n",
    "                if offset[0] == 0:\n",
    "                    aligned_labels.append(labels[current_label_index])\n",
    "                    current_label_index += 1\n",
    "                else:\n",
    "                    # If the token is not the first subtoken, you can decide how to label it. \n",
    "                    # For simplicity, let's use the same label as the first subtoken\n",
    "                    aligned_labels.append(labels[current_label_index - 1])\n",
    "\n",
    "        return aligned_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "378ec055",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:08:25.691763Z",
     "iopub.status.busy": "2024-11-18T20:08:25.691405Z",
     "iopub.status.idle": "2024-11-18T20:08:25.694450Z",
     "shell.execute_reply": "2024-11-18T20:08:25.693940Z"
    },
    "papermill": {
     "duration": 0.011031,
     "end_time": "2024-11-18T20:08:25.695984",
     "exception": false,
     "start_time": "2024-11-18T20:08:25.684953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create train dataset and test dataset\n",
    "train_dataset = NER_Dataset(train_data, tokenizer)\n",
    "test_dataset = NER_Dataset(test_data, tokenizer)\n",
    "\n",
    "# Create train dataloader and test dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a1c6af0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:08:25.708465Z",
     "iopub.status.busy": "2024-11-18T20:08:25.708045Z",
     "iopub.status.idle": "2024-11-18T20:08:25.758924Z",
     "shell.execute_reply": "2024-11-18T20:08:25.758412Z"
    },
    "papermill": {
     "duration": 0.059065,
     "end_time": "2024-11-18T20:08:25.760675",
     "exception": false,
     "start_time": "2024-11-18T20:08:25.701610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "104d7fc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:08:25.773915Z",
     "iopub.status.busy": "2024-11-18T20:08:25.773590Z",
     "iopub.status.idle": "2024-11-18T20:08:25.777637Z",
     "shell.execute_reply": "2024-11-18T20:08:25.777128Z"
    },
    "papermill": {
     "duration": 0.012193,
     "end_time": "2024-11-18T20:08:25.779103",
     "exception": false,
     "start_time": "2024-11-18T20:08:25.766910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super(CrossAttentionFusion, self).__init__()\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, embeddings_mbert, embeddings_indicbert):\n",
    "        \"\"\"\n",
    "        Fuse embeddings from mBERT and Indic-BERT using cross-attention.\n",
    "\n",
    "        Args:\n",
    "            embeddings_mbert (torch.Tensor): Embeddings from mBERT of shape [batch_size, seq_len_m, hidden_dim].\n",
    "            embeddings_indicbert (torch.Tensor): Embeddings from Indic-BERT of shape [batch_size, seq_len_i, hidden_dim].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Fused embeddings of shape [batch_size, seq_len_m, hidden_dim].\n",
    "        \"\"\"\n",
    "        embeddings_mbert = embeddings_mbert.to(DEVICE)\n",
    "        embeddings_indicbert = embeddings_indicbert.to(DEVICE)\n",
    "        \n",
    "        # Transpose to shape [seq_len, batch_size, hidden_dim] for nn.MultiheadAttention\n",
    "        embeddings_mbert = embeddings_mbert.transpose(0, 1)\n",
    "        embeddings_indicbert = embeddings_indicbert.transpose(0, 1)\n",
    "\n",
    "        # Cross-attention: Indic BERT queries attend to m-BERT keys and values\n",
    "        attn_output, _ = self.cross_attention(\n",
    "            query=embeddings_indicbert,       # Queries from Indic-BERT\n",
    "            key=embeddings_mbert,     # Keys from m-BERT\n",
    "            value=embeddings_mbert    # Values from m-BERT\n",
    "        )\n",
    "\n",
    "        attn_output.to(DEVICE)\n",
    "\n",
    "        # Residual connection and layer normalization\n",
    "        output = self.layer_norm(attn_output + embeddings_mbert)\n",
    "\n",
    "        # Transpose back to [batch_size, seq_len_m, hidden_dim]\n",
    "        output = output.transpose(0, 1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98c7f5d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:08:25.792714Z",
     "iopub.status.busy": "2024-11-18T20:08:25.792283Z",
     "iopub.status.idle": "2024-11-18T20:08:25.794904Z",
     "shell.execute_reply": "2024-11-18T20:08:25.794345Z"
    },
    "papermill": {
     "duration": 0.011164,
     "end_time": "2024-11-18T20:08:25.796532",
     "exception": false,
     "start_time": "2024-11-18T20:08:25.785368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hidden_dim = 768\n",
    "num_heads = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b16339aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:08:25.809954Z",
     "iopub.status.busy": "2024-11-18T20:08:25.809497Z",
     "iopub.status.idle": "2024-11-18T20:08:28.201149Z",
     "shell.execute_reply": "2024-11-18T20:08:28.200599Z"
    },
    "papermill": {
     "duration": 2.400088,
     "end_time": "2024-11-18T20:08:28.202720",
     "exception": false,
     "start_time": "2024-11-18T20:08:25.802632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBERT_NER(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (indic_bert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(200000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertSdpaAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (attention): CrossAttentionFusion(\n",
       "    (cross_attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (gru): GRU(2304, 128, batch_first=True)\n",
       "  (classifier): Linear(in_features=128, out_features=7, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model for NER\n",
    "\n",
    "class MBERT_NER(nn.Module):\n",
    "    def __init__(self, num_labels, gru_hidden_size, num_gru_layers, freeze_bert=False):\n",
    "        super(MBERT_NER, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "        self.indic_bert = AutoModel.from_pretrained(\"ai4bharat/indic-bert\")\n",
    "        \n",
    "        # Initialize the Fusion Attention\n",
    "        self.attention = CrossAttentionFusion(hidden_dim=hidden_dim, num_heads=num_heads).to(DEVICE)\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=self.bert.config.hidden_size * 3,\n",
    "                          hidden_size=gru_hidden_size,\n",
    "                          num_layers=num_gru_layers,\n",
    "                          batch_first=True)\n",
    "        self.classifier = nn.Linear(gru_hidden_size, num_labels)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.batch_norm = nn.BatchNorm1d(gru_hidden_size)\n",
    "\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.indic_bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \n",
    "        input_ids = input_ids.to(DEVICE)\n",
    "        attention_mask = attention_mask.to(DEVICE)\n",
    "        \n",
    "        # Embeddings\n",
    "        outputs1 = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        outputs2 = self.indic_bert(input_ids = input_ids, attention_mask = attention_mask)        \n",
    "        \n",
    "        embeddings_mbert = outputs1.last_hidden_state.to(DEVICE)\n",
    "        embeddings_indicbert = outputs2.last_hidden_state.to(DEVICE)\n",
    "        \n",
    "        # Fused embeddings\n",
    "        embeddings = self.attention(embeddings_mbert, embeddings_indicbert).to(DEVICE)\n",
    "        \n",
    "        sequence_output = torch.cat((embeddings_mbert, embeddings_indicbert, embeddings), dim=-1)\n",
    "        \n",
    "        gru_output, _ = self.gru(sequence_output)\n",
    "        gru_output = self.batch_norm(gru_output)\n",
    "        gru_output = self.dropout(gru_output)\n",
    "        \n",
    "        logits = self.classifier(gru_output)\n",
    "        return logits\n",
    "\n",
    "# Create the NER model\n",
    "NUM_LABELS = 7 # Number of NER tags\n",
    "GRU_HIDDEN_SIZE = 128 # Hidden size of the GRU\n",
    "NUM_GRU_LAYERS = 1 # Number of layers in the GRU\n",
    "FREEZE_BERT = False # Whether to freeze the BERT model\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = MBERT_NER(num_labels=NUM_LABELS,\n",
    "                    gru_hidden_size=GRU_HIDDEN_SIZE,\n",
    "                    num_gru_layers=NUM_GRU_LAYERS,\n",
    "                    freeze_bert=FREEZE_BERT)\n",
    "\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cced4411",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:08:28.216530Z",
     "iopub.status.busy": "2024-11-18T20:08:28.216267Z",
     "iopub.status.idle": "2024-11-18T20:08:28.218758Z",
     "shell.execute_reply": "2024-11-18T20:08:28.218279Z"
    },
    "papermill": {
     "duration": 0.010849,
     "end_time": "2024-11-18T20:08:28.220255",
     "exception": false,
     "start_time": "2024-11-18T20:08:28.209406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8bf6ba3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:08:28.234045Z",
     "iopub.status.busy": "2024-11-18T20:08:28.233719Z",
     "iopub.status.idle": "2024-11-18T20:15:13.424663Z",
     "shell.execute_reply": "2024-11-18T20:15:13.423799Z"
    },
    "papermill": {
     "duration": 405.20252,
     "end_time": "2024-11-18T20:15:13.429097",
     "exception": false,
     "start_time": "2024-11-18T20:08:28.226577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6, Train Loss: 0.5446, Train F1: 0.7499, Train Acc: 0.8304, Val Loss: 0.3099, Val F1: 0.8536, Val Acc: 0.9050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/6, Train Loss: 0.2632, Train F1: 0.8842, Train Acc: 0.9225, Val Loss: 0.2542, Val F1: 0.8866, Val Acc: 0.9244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/6, Train Loss: 0.1785, Train F1: 0.9203, Train Acc: 0.9475, Val Loss: 0.2688, Val F1: 0.8906, Val Acc: 0.9255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/6, Train Loss: 0.1378, Train F1: 0.9388, Train Acc: 0.9606, Val Loss: 0.2584, Val F1: 0.9027, Val Acc: 0.9321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/6, Train Loss: 0.1025, Train F1: 0.9571, Train Acc: 0.9720, Val Loss: 0.3064, Val F1: 0.8875, Val Acc: 0.9241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/6, Train Loss: 0.0893, Train F1: 0.9621, Train Acc: 0.9758, Val Loss: 0.2566, Val F1: 0.9087, Val Acc: 0.9369\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 6\n",
    "\n",
    "train_losses = []\n",
    "train_f1_scores = []\n",
    "train_acc_scores = []\n",
    "val_losses = []\n",
    "val_f1_scores = []\n",
    "val_acc_scores = []\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_predictions, train_labels = [], []\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        logits = logits.view(-1, NUM_LABELS)\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Get predictions and filter out ignored indices for metric calculations\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        active_indices = labels != -100\n",
    "        train_predictions.extend(predictions[active_indices].cpu().numpy())\n",
    "        train_labels.extend(labels[active_indices].cpu().numpy())\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    train_f1 = f1_score(train_labels, train_predictions, average='macro')\n",
    "    train_f1_scores.append(train_f1)\n",
    "\n",
    "    train_acc = accuracy_score(train_labels, train_predictions)\n",
    "    train_acc_scores.append(train_acc)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_predictions, val_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            logits = logits.view(-1, NUM_LABELS)\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Filter predictions and labels\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            active_indices = labels != -100\n",
    "            val_predictions.extend(predictions[active_indices].cpu().numpy())\n",
    "            val_labels.extend(labels[active_indices].cpu().numpy())\n",
    "\n",
    "    val_loss /= len(test_dataloader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    val_f1 = f1_score(val_labels, val_predictions, average='macro')\n",
    "    val_f1_scores.append(val_f1)\n",
    "\n",
    "    val_acc = accuracy_score(val_labels, val_predictions)\n",
    "    val_acc_scores.append(val_acc)\n",
    "                                                \n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9fd5814",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:15:13.438080Z",
     "iopub.status.busy": "2024-11-18T20:15:13.437818Z",
     "iopub.status.idle": "2024-11-18T20:15:27.673421Z",
     "shell.execute_reply": "2024-11-18T20:15:27.672768Z"
    },
    "papermill": {
     "duration": 14.240927,
     "end_time": "2024-11-18T20:15:27.674510",
     "exception": false,
     "start_time": "2024-11-18T20:15:13.433583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: Hindi\n",
      "\n",
      "Weighted F1: 0.9347\n",
      "Macro F1: 0.9163\n",
      "Accuracy: 0.9347\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97      5277\n",
      "           1       0.93      0.94      0.94      1481\n",
      "           2       0.93      0.94      0.94      1874\n",
      "           3       0.87      0.91      0.89       986\n",
      "           4       0.91      0.94      0.93      2602\n",
      "           5       0.91      0.88      0.90      1195\n",
      "           6       0.89      0.84      0.86       867\n",
      "\n",
      "    accuracy                           0.93     14282\n",
      "   macro avg       0.92      0.92      0.92     14282\n",
      "weighted avg       0.94      0.93      0.93     14282\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[5060   33   37   29   39   54   25]\n",
      " [  17 1389   17   42    2   14    0]\n",
      " [  14   18 1762    2   66    0   12]\n",
      " [  17   36    2  900    5   25    1]\n",
      " [  49    3   47    0 2458   11   34]\n",
      " [  30   11    4   65   11 1056   18]\n",
      " [  13    0   18    0  108    3  725]]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: Bengali\n",
      "\n",
      "Weighted F1: 0.8033\n",
      "Macro F1: 0.8043\n",
      "Accuracy: 0.8069\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.94      0.87      1835\n",
      "           1       0.77      0.89      0.82      1343\n",
      "           2       0.84      0.85      0.85      2490\n",
      "           3       0.78      0.85      0.81      1239\n",
      "           4       0.76      0.83      0.79      3068\n",
      "           5       0.82      0.74      0.78      1510\n",
      "           6       0.90      0.58      0.71      2080\n",
      "\n",
      "    accuracy                           0.81     13565\n",
      "   macro avg       0.81      0.81      0.80     13565\n",
      "weighted avg       0.81      0.81      0.80     13565\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[1719    9    4   25   43   25   10]\n",
      " [  31 1190   16   44    0   62    0]\n",
      " [ 163   46 2124    1  100    7   49]\n",
      " [  11  113    0 1049    8   58    0]\n",
      " [ 130   15  269   22 2539   15   78]\n",
      " [  19  167    0  198   10 1115    1]\n",
      " [  56   13  106    1  622   72 1210]]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: Marathi\n",
      "\n",
      "Weighted F1: 0.9385\n",
      "Macro F1: 0.9015\n",
      "Accuracy: 0.9385\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97     10408\n",
      "           1       0.92      0.94      0.93      1342\n",
      "           2       0.95      0.96      0.96      1992\n",
      "           3       0.80      0.86      0.83      1172\n",
      "           4       0.90      0.96      0.93      2535\n",
      "           5       0.89      0.88      0.89      1829\n",
      "           6       0.86      0.76      0.81       820\n",
      "\n",
      "    accuracy                           0.94     20098\n",
      "   macro avg       0.90      0.90      0.90     20098\n",
      "weighted avg       0.94      0.94      0.94     20098\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[10014    53    25    80    87   108    41]\n",
      " [   31  1255     8    36     3     9     0]\n",
      " [   19    12  1922     2    36     0     1]\n",
      " [   65    23     0  1011    11    56     6]\n",
      " [   28     0    41     2  2425     1    38]\n",
      " [   55    14     4   130     1  1612    13]\n",
      " [   22     0    15     0   142    19   622]]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: Tamil\n",
      "\n",
      "Weighted F1: 0.8276\n",
      "Macro F1: 0.7313\n",
      "Accuracy: 0.8259\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.89      0.92     11461\n",
      "           1       0.72      0.73      0.72      1643\n",
      "           2       0.84      0.85      0.85      2295\n",
      "           3       0.55      0.52      0.54      1512\n",
      "           4       0.72      0.81      0.77      2532\n",
      "           5       0.72      0.82      0.77      1881\n",
      "           6       0.55      0.59      0.57       677\n",
      "\n",
      "    accuracy                           0.83     22001\n",
      "   macro avg       0.72      0.74      0.73     22001\n",
      "weighted avg       0.83      0.83      0.83     22001\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[10242   289    72   395   204   173    86]\n",
      " [   86  1194   193    91    21    58     0]\n",
      " [  120   110  1949    18    74     0    24]\n",
      " [   90    49    21   788   284   272     8]\n",
      " [  179     2    50    33  2059    55   154]\n",
      " [  113    25     6   104    38  1541    54]\n",
      " [   53     0    26     0   166    35   397]]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: Telugu\n",
      "\n",
      "Weighted F1: 0.8403\n",
      "Macro F1: 0.6937\n",
      "Accuracy: 0.8343\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.89      0.93     17291\n",
      "           1       0.63      0.71      0.66      1810\n",
      "           2       0.69      0.91      0.79      1353\n",
      "           3       0.34      0.38      0.36      1375\n",
      "           4       0.74      0.76      0.75      2269\n",
      "           5       0.73      0.86      0.79      2072\n",
      "           6       0.52      0.65      0.58       455\n",
      "\n",
      "    accuracy                           0.83     26625\n",
      "   macro avg       0.66      0.74      0.69     26625\n",
      "weighted avg       0.85      0.83      0.84     26625\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[15369   357   211   633   286   280   155]\n",
      " [  156  1282   167   132    17    56     0]\n",
      " [   21    38  1231     0    50     0    13]\n",
      " [  123   329    16   522   184   189    12]\n",
      " [  101     0   132   150  1725    95    66]\n",
      " [  101    43     0   108    10  1785    25]\n",
      " [   31     0    20     0    60    46   298]]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load the test data from json for all 5 languages\n",
    "languages = ['hindi', 'bengali', 'marathi', 'tamil', 'telugu']\n",
    "\n",
    "# Iterate over all languages and evaluate the model\n",
    "for lang in languages:\n",
    "    with open(f\"/home/vijay/slim_sense/SHARPax/Python_code/test/NER-Experiments/Dataset/{lang}_test.json\") as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    test_dataset = NER_Dataset(test_data, tokenizer)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    test_predictions, test_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            logits = logits.view(-1, NUM_LABELS)\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            # Filter predictions and labels\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            active_indices = labels != -100\n",
    "            test_predictions.extend(predictions[active_indices].cpu().numpy())\n",
    "            test_labels.extend(labels[active_indices].cpu().numpy())\n",
    "\n",
    "    weighted_f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
    "    macro_f1 = f1_score(test_labels, test_predictions, average='macro')\n",
    "    accuracy = accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "    if lang == 'hindi':\n",
    "        LANG = \"Hindi\"\n",
    "    elif lang == 'bengali':\n",
    "        LANG = \"Bengali\"\n",
    "    elif lang == 'marathi':\n",
    "        LANG = \"Marathi\"\n",
    "    elif lang == 'tamil':\n",
    "        LANG = \"Tamil\"\n",
    "    elif lang == 'telugu':\n",
    "        LANG = \"Telugu\"\n",
    "\n",
    "    print(f\"Language: {LANG}\")\n",
    "    print()\n",
    "    print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "    print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print()\n",
    "    print(\"CLASSIFICATION REPORT: \")\n",
    "    print(classification_report(test_labels, test_predictions))\n",
    "    print()\n",
    "    print(\"CONFUSION MATRIX: \")\n",
    "    print(confusion_matrix(test_labels, test_predictions))\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4343eadd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:15:27.685289Z",
     "iopub.status.busy": "2024-11-18T20:15:27.685086Z",
     "iopub.status.idle": "2024-11-18T20:15:29.019121Z",
     "shell.execute_reply": "2024-11-18T20:15:29.018363Z"
    },
    "papermill": {
     "duration": 1.340937,
     "end_time": "2024-11-18T20:15:29.020437",
     "exception": false,
     "start_time": "2024-11-18T20:15:27.679500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), f'NER_FineTune_{lang1[:2]}_{lang2[:2]}_cross_attn.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707c56c5",
   "metadata": {
    "papermill": {
     "duration": 0.00501,
     "end_time": "2024-11-18T20:15:29.030322",
     "exception": false,
     "start_time": "2024-11-18T20:15:29.025312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f76d60",
   "metadata": {
    "papermill": {
     "duration": 0.003848,
     "end_time": "2024-11-18T20:15:29.038830",
     "exception": false,
     "start_time": "2024-11-18T20:15:29.034982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DUMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c204a",
   "metadata": {
    "papermill": {
     "duration": 0.0045,
     "end_time": "2024-11-18T20:15:29.047935",
     "exception": false,
     "start_time": "2024-11-18T20:15:29.043435",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbd52da9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:15:29.058830Z",
     "iopub.status.busy": "2024-11-18T20:15:29.058698Z",
     "iopub.status.idle": "2024-11-18T20:15:29.061814Z",
     "shell.execute_reply": "2024-11-18T20:15:29.061268Z"
    },
    "papermill": {
     "duration": 0.00918,
     "end_time": "2024-11-18T20:15:29.062706",
     "exception": false,
     "start_time": "2024-11-18T20:15:29.053526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class FusionModule(nn.Module):\n",
    "#     def __init__(self, hidden_size):\n",
    "#         super(FusionModule, self).__init__()\n",
    "#         # Linear layer to compute attention scores\n",
    "#         self.attention_layer = nn.Linear(hidden_size * 2, 2)\n",
    "\n",
    "#     def forward(self, embeddings_mbert, embeddings_indicbert):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             embeddings_mbert (torch.Tensor): [batch_size, seq_len, hidden_size]\n",
    "#             embeddings_indicbert (torch.Tensor): [batch_size, seq_len, hidden_size]\n",
    "#         Returns:\n",
    "#             torch.Tensor: Fused embeddings [batch_size, seq_len, hidden_size]\n",
    "#         \"\"\"\n",
    "#         # Concatenate embeddings along the last dimension\n",
    "#         concat_embeddings = torch.cat((embeddings_mbert, embeddings_indicbert), dim=-1)  # [batch_size, seq_len, hidden_size * 2]\n",
    "\n",
    "#         # Compute attention scores\n",
    "#         attn_scores = self.attention_layer(concat_embeddings)  # [batch_size, seq_len, 2]\n",
    "\n",
    "#         # Apply softmax to get attention weights\n",
    "#         attn_weights = torch.softmax(attn_scores, dim=-1)  # [batch_size, seq_len, 2]\n",
    "\n",
    "#         # Split attention weights\n",
    "#         attn_weights_mbert = attn_weights[..., 0].unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
    "#         attn_weights_indicbert = attn_weights[..., 1].unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
    "\n",
    "#         # Compute weighted sum of embeddings\n",
    "#         fused_embeddings = attn_weights_mbert * embeddings_mbert + attn_weights_indicbert * embeddings_indicbert  # [batch_size, seq_len, hidden_size]\n",
    "\n",
    "#         return fused_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e2e1ad0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:15:29.069635Z",
     "iopub.status.busy": "2024-11-18T20:15:29.069489Z",
     "iopub.status.idle": "2024-11-18T20:15:29.072511Z",
     "shell.execute_reply": "2024-11-18T20:15:29.072023Z"
    },
    "papermill": {
     "duration": 0.007461,
     "end_time": "2024-11-18T20:15:29.073297",
     "exception": false,
     "start_time": "2024-11-18T20:15:29.065836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class SelfAttentionLayer(nn.Module):\n",
    "#     def __init__(self, hidden_dim, num_heads, dropout=0.1):\n",
    "#         super(SelfAttentionLayer, self).__init__()\n",
    "#         self.self_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, dropout=dropout)\n",
    "#         self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, embeddings, attention_mask=None):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             embeddings (torch.Tensor): [batch_size, seq_len, hidden_dim]\n",
    "#             attention_mask (torch.Tensor): [batch_size, seq_len], with 1 for tokens to attend to and 0 for padding\n",
    "#         Returns:\n",
    "#             torch.Tensor: [batch_size, seq_len, hidden_dim]\n",
    "#         \"\"\"\n",
    "#         embeddings = embeddings.to(DEVICE)\n",
    "        \n",
    "        \n",
    "#         # Transpose embeddings to [seq_len, batch_size, hidden_dim] for nn.MultiheadAttention\n",
    "#         embeddings = embeddings.transpose(0, 1)\n",
    "\n",
    "#         # Prepare attention mask for nn.MultiheadAttention (optional)\n",
    "#         if attention_mask is not None:\n",
    "#     # nn.MultiheadAttention expects key_padding_mask of shape [batch_size, seq_len]\n",
    "#             key_padding_mask = attention_mask == 0  # True for padding tokens\n",
    "#         else:\n",
    "#             key_padding_mask = None\n",
    "\n",
    "\n",
    "#         # Self-attention\n",
    "#         attn_output, _ = self.self_attention(\n",
    "#             query=embeddings,\n",
    "#             key=embeddings,\n",
    "#             value=embeddings,\n",
    "#             key_padding_mask=key_padding_mask\n",
    "#         )\n",
    "\n",
    "#         # Residual connection and layer normalization\n",
    "#         attn_output = self.dropout(attn_output)\n",
    "#         attn_output = self.layer_norm(attn_output + embeddings)\n",
    "\n",
    "#         # Transpose back to [batch_size, seq_len, hidden_dim]\n",
    "#         attn_output = attn_output.transpose(0, 1)\n",
    "\n",
    "#         return attn_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 429.895216,
   "end_time": "2024-11-18T20:15:30.395091",
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/vijay/slim_sense/SHARPax/Python_code/test/NER-Experiments/FineTune-Experiments-TwoLang/FineTune-Hindi-Bengali.ipynb",
   "output_path": "/home/vijay/slim_sense/SHARPax/Python_code/test/NER-Experiments/FineTune-Experiments-TwoLang//FineTune-hindi-marathi.ipynb",
   "parameters": {
    "lang1": "hindi",
    "lang2": "marathi"
   },
   "start_time": "2024-11-18T20:08:20.499875",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
