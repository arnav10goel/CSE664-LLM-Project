{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load hindi dataset\n",
    "hindi_dataset = load_dataset(\"wikiann\", \"hi\")\n",
    "\n",
    "# Load marathi data\n",
    "marathi_dataset = load_dataset(\"wikiann\", \"mr\")\n",
    "\n",
    "# Load bengali data\n",
    "bengali_dataset = load_dataset(\"wikiann\", \"bn\")\n",
    "\n",
    "# Load tamil data\n",
    "tamil_dataset = load_dataset(\"wikiann\", \"ta\")\n",
    "\n",
    "# Load telugu data\n",
    "telugu_dataset = load_dataset(\"wikiann\", \"te\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi: \n",
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Print the datasets\n",
    "print(\"Hindi: \")\n",
    "print(hindi_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marathi: \n",
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Marathi: \")\n",
    "print(marathi_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali: \n",
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Bengali: \")\n",
    "print(bengali_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamil: \n",
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 15000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamil: \")\n",
    "print(tamil_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telugu: \n",
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Telugu: \")\n",
    "print(telugu_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hindi Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of tokens:  77\n"
     ]
    }
   ],
   "source": [
    "# Get the train data for Hindi\n",
    "hindi_train = hindi_dataset['train']\n",
    "\n",
    "# Get the test data for Hindi\n",
    "hindi_test = hindi_dataset['test']\n",
    "\n",
    "hindi_train_lst = []\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "# For each example in the train data, get the tokens and NER tags\n",
    "for example in hindi_train:\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "\n",
    "    # Check if the tokens and NER tags are of the same length\n",
    "    # If not then skip this example\n",
    "    if len(tokens) != len(ner_tags):\n",
    "        continue\n",
    "\n",
    "    # Check if they are not of length 0\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    max_len = max(max_len, len(tokens))\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_dict['tokens'] = tokens\n",
    "    temp_dict['ner_tags'] = ner_tags\n",
    "    hindi_train_lst.append(temp_dict)\n",
    "\n",
    "hindi_test_lst = []\n",
    "\n",
    "# For each example in the test data, get the tokens and NER tags\n",
    "for example in hindi_test:\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "\n",
    "    # Check if the tokens and NER tags are of the same length\n",
    "    # If not then skip this example\n",
    "    if len(tokens) != len(ner_tags):\n",
    "        continue\n",
    "\n",
    "    # Check if they are not of length 0\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    max_len = max(max_len, len(tokens))\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_dict['tokens'] = tokens\n",
    "    temp_dict['ner_tags'] = ner_tags\n",
    "    hindi_test_lst.append(temp_dict)\n",
    "\n",
    "print(\"Max length of tokens: \", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 4000 examples from the train data\n",
    "import random\n",
    "random.shuffle(hindi_train_lst)\n",
    "hindi_train_lst = hindi_train_lst[:4000]\n",
    "\n",
    "# Randomly select 1000 examples from the test data\n",
    "random.shuffle(hindi_test_lst)\n",
    "hindi_test_lst = hindi_test_lst[:1000]\n",
    "\n",
    "# Dump the train and test data to a json file\n",
    "import json\n",
    "\n",
    "with open('hindi_train.json', 'w') as f:\n",
    "    json.dump(hindi_train_lst, f, indent=4)\n",
    "\n",
    "with open('hindi_test.json', 'w') as f:\n",
    "    json.dump(hindi_test_lst, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Marathi Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "1000\n",
      "Max length of tokens:  60\n"
     ]
    }
   ],
   "source": [
    "# Get the train data for Marathi\n",
    "marathi_train = marathi_dataset['train']\n",
    "\n",
    "# Get the test data for Marathi\n",
    "marathi_test = marathi_dataset['test']\n",
    "\n",
    "marathi_train_lst = []\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "# For each example in the train data, get the tokens and NER tags\n",
    "for example in marathi_train:\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "\n",
    "    # Check if the tokens and NER tags are of the same length\n",
    "    # If not then skip this example\n",
    "    if len(tokens) != len(ner_tags):\n",
    "        continue\n",
    "\n",
    "    # Check if they are not of length 0\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    max_len = max(max_len, len(tokens))\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_dict['tokens'] = tokens\n",
    "    temp_dict['ner_tags'] = ner_tags\n",
    "    marathi_train_lst.append(temp_dict)\n",
    "\n",
    "marathi_test_lst = []\n",
    "\n",
    "# For each example in the test data, get the tokens and NER tags\n",
    "for example in marathi_test:\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "\n",
    "    # Check if the tokens and NER tags are of the same length\n",
    "    # If not then skip this example\n",
    "    if len(tokens) != len(ner_tags):\n",
    "        continue\n",
    "\n",
    "    # Check if they are not of length 0\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    max_len = max(max_len, len(tokens))\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_dict['tokens'] = tokens\n",
    "    temp_dict['ner_tags'] = ner_tags\n",
    "    marathi_test_lst.append(temp_dict)\n",
    "\n",
    "print(len(marathi_train_lst))\n",
    "print(len(marathi_test_lst))\n",
    "print(\"Max length of tokens: \", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 4000 examples from the train data\n",
    "random.shuffle(marathi_train_lst)\n",
    "marathi_train_lst = marathi_train_lst[:4000]\n",
    "\n",
    "# Randomly select 1000 examples from the test data\n",
    "random.shuffle(marathi_test_lst)\n",
    "marathi_test_lst = marathi_test_lst[:1000]\n",
    "\n",
    "# Dump the train and test data to a json file\n",
    "with open('marathi_train.json', 'w') as f:\n",
    "    json.dump(marathi_train_lst, f, indent=4)\n",
    "\n",
    "with open('marathi_test.json', 'w') as f:\n",
    "    json.dump(marathi_test_lst, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bengali Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "1000\n",
      "Max length of tokens:  62\n"
     ]
    }
   ],
   "source": [
    "# Get the train data for Bengali\n",
    "bengali_train = bengali_dataset['train']\n",
    "\n",
    "# Get the test data for Bengali\n",
    "bengali_test = bengali_dataset['test']\n",
    "\n",
    "bengali_train_lst = []\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "# For each example in the train data, get the tokens and NER tags\n",
    "for example in bengali_train:\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "\n",
    "    # Check if the tokens and NER tags are of the same length\n",
    "    # If not then skip this example\n",
    "    if len(tokens) != len(ner_tags):\n",
    "        continue\n",
    "\n",
    "    # Check if they are not of length 0\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    max_len = max(max_len, len(tokens))\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_dict['tokens'] = tokens\n",
    "    temp_dict['ner_tags'] = ner_tags\n",
    "    bengali_train_lst.append(temp_dict)\n",
    "\n",
    "bengali_test_lst = []\n",
    "\n",
    "# For each example in the test data, get the tokens and NER tags\n",
    "for example in bengali_test:\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "\n",
    "    # Check if the tokens and NER tags are of the same length\n",
    "    # If not then skip this example\n",
    "    if len(tokens) != len(ner_tags):\n",
    "        continue\n",
    "\n",
    "    # Check if they are not of length 0\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    max_len = max(max_len, len(tokens))\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_dict['tokens'] = tokens\n",
    "    temp_dict['ner_tags'] = ner_tags\n",
    "    bengali_test_lst.append(temp_dict)\n",
    "\n",
    "print(len(bengali_train_lst))\n",
    "print(len(bengali_test_lst))\n",
    "print(\"Max length of tokens: \", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 4000 examples from the train data\n",
    "random.shuffle(bengali_train_lst)\n",
    "bengali_train_lst = bengali_train_lst[:4000]\n",
    "\n",
    "# Randomly select 1000 examples from the test data\n",
    "random.shuffle(bengali_test_lst)\n",
    "bengali_test_lst = bengali_test_lst[:1000]\n",
    "\n",
    "# Dump the train and test data to a json file\n",
    "with open('bengali_train.json', 'w') as f:\n",
    "    json.dump(bengali_train_lst, f, indent=4)\n",
    "\n",
    "with open('bengali_test.json', 'w') as f:\n",
    "    json.dump(bengali_test_lst, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tamil Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "1000\n",
      "Max length of tokens:  46\n"
     ]
    }
   ],
   "source": [
    "# Get the train data for Tamil\n",
    "tamil_train = tamil_dataset['train']\n",
    "\n",
    "# Get the test data for Tamil\n",
    "tamil_test = tamil_dataset['test']\n",
    "\n",
    "tamil_train_lst = []\n",
    "\n",
    "max_len = 0 \n",
    "\n",
    "# For each example in the train data, get the tokens and NER tags\n",
    "for example in tamil_train:\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "\n",
    "    # Check if the tokens and NER tags are of the same length\n",
    "    # If not then skip this example\n",
    "    if len(tokens) != len(ner_tags):\n",
    "        continue\n",
    "\n",
    "    # Check if they are not of length 0\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    max_len = max(max_len, len(tokens))\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_dict['tokens'] = tokens\n",
    "    temp_dict['ner_tags'] = ner_tags\n",
    "    tamil_train_lst.append(temp_dict)\n",
    "\n",
    "tamil_test_lst = []\n",
    "\n",
    "# For each example in the test data, get the tokens and NER tags\n",
    "for example in tamil_test:\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "\n",
    "    # Check if the tokens and NER tags are of the same length\n",
    "    # If not then skip this example\n",
    "    if len(tokens) != len(ner_tags):\n",
    "        continue\n",
    "\n",
    "    # Check if they are not of length 0\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    max_len = max(max_len, len(tokens))\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_dict['tokens'] = tokens\n",
    "    temp_dict['ner_tags'] = ner_tags\n",
    "    tamil_test_lst.append(temp_dict)\n",
    "\n",
    "print(len(tamil_train_lst))\n",
    "print(len(tamil_test_lst))\n",
    "print(\"Max length of tokens: \", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 4000 examples from the train data\n",
    "random.shuffle(tamil_train_lst)\n",
    "tamil_train_lst = tamil_train_lst[:4000]\n",
    "\n",
    "# Randomly select 1000 examples from the test data\n",
    "random.shuffle(tamil_test_lst)\n",
    "tamil_test_lst = tamil_test_lst[:1000]\n",
    "\n",
    "# Dump the train and test data to a json file\n",
    "with open('tamil_train.json', 'w') as f:\n",
    "    json.dump(tamil_train_lst, f, indent=4)\n",
    "\n",
    "with open('tamil_test.json', 'w') as f:\n",
    "    json.dump(tamil_test_lst, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Telugu Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "1000\n",
      "Max length of tokens:  42\n"
     ]
    }
   ],
   "source": [
    "# Get the train data for Telugu\n",
    "telugu_train = telugu_dataset['train']\n",
    "\n",
    "# Get the valid data for Telugu\n",
    "telugu_valid = telugu_dataset['validation']\n",
    "\n",
    "telugu_train_lst = []\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "# For each example in the train data, get the tokens and NER tags\n",
    "for example in telugu_train:\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "\n",
    "    # Check if the tokens and NER tags are of the same length\n",
    "    # If not then skip this example\n",
    "    if len(tokens) != len(ner_tags):\n",
    "        continue\n",
    "\n",
    "    # Check if they are not of length 0\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    max_len = max(max_len, len(tokens))\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_dict['tokens'] = tokens\n",
    "    temp_dict['ner_tags'] = ner_tags\n",
    "    telugu_train_lst.append(temp_dict)\n",
    "\n",
    "# Also append everything in validation to train\n",
    "for example in telugu_valid:\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "\n",
    "    # Check if the tokens and NER tags are of the same length\n",
    "    # If not then skip this example\n",
    "    if len(tokens) != len(ner_tags):\n",
    "        continue\n",
    "\n",
    "    # Check if they are not of length 0\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    max_len = max(max_len, len(tokens))\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_dict['tokens'] = tokens\n",
    "    temp_dict['ner_tags'] = ner_tags\n",
    "    telugu_train_lst.append(temp_dict)\n",
    "\n",
    "# Get the test data for Telugu\n",
    "telugu_test = telugu_dataset['test']\n",
    "\n",
    "telugu_test_lst = []\n",
    "\n",
    "# For each example in the test data, get the tokens and NER tags\n",
    "for example in telugu_test:\n",
    "    tokens = example['tokens']\n",
    "    ner_tags = example['ner_tags']\n",
    "\n",
    "    # Check if the tokens and NER tags are of the same length\n",
    "    # If not then skip this example\n",
    "    if len(tokens) != len(ner_tags):\n",
    "        continue\n",
    "\n",
    "    # Check if they are not of length 0\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    max_len = max(max_len, len(tokens))\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_dict['tokens'] = tokens\n",
    "    temp_dict['ner_tags'] = ner_tags\n",
    "    telugu_test_lst.append(temp_dict)\n",
    "\n",
    "print(len(telugu_train_lst))\n",
    "print(len(telugu_test_lst))\n",
    "print(\"Max length of tokens: \", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 2000 examples from the train data\n",
    "random.shuffle(telugu_train_lst)\n",
    "telugu_train_lst = telugu_train_lst[:2000]\n",
    "\n",
    "# Randomly select 1000 examples from the test data\n",
    "random.shuffle(telugu_test_lst)\n",
    "telugu_test_lst = telugu_test_lst[:1000]\n",
    "\n",
    "# Dump the train and test data to a json file\n",
    "with open('telugu_train.json', 'w') as f:\n",
    "    json.dump(telugu_train_lst, f, indent=4)\n",
    "\n",
    "with open('telugu_test.json', 'w') as f:\n",
    "    json.dump(telugu_test_lst, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLA3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
