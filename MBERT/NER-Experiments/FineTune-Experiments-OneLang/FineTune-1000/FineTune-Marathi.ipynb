{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import string\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data from json\n",
    "with open('/home/arnav/BTP/Arnav_Medha/CSE556-NLP-Project/NER-Experiments/Dataset/marathi_train.json') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Load test data from json\n",
    "with open('/home/arnav/BTP/Arnav_Medha/CSE556-NLP-Project/NER-Experiments/Dataset/marathi_test.json') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Take the first 1000 samples for training\n",
    "train_data = train_data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokeniser for MBERT\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER_Dataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=128):\n",
    "        self.data = data  # List of sentences and labels\n",
    "        self.tokenizer = tokenizer  # mBERT tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx][\"tokens\"]\n",
    "        word_labels = self.data[idx][\"ner_tags\"]\n",
    "\n",
    "        # Tokenize the text and align the labels\n",
    "        encoding = self.tokenizer(text,\n",
    "                                  is_split_into_words=True,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=self.max_len)\n",
    "\n",
    "        labels = [self.align_labels(encoding['offset_mapping'], word_labels)]\n",
    "\n",
    "        # Remove the offset mapping to prevent issues during model training\n",
    "        del encoding['offset_mapping']\n",
    "\n",
    "        item = {key: torch.tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels[0], dtype=torch.long)\n",
    "\n",
    "        return item\n",
    "    \n",
    "    # Create a function to align labels\n",
    "    def align_labels(self, offset_mapping, labels):\n",
    "        aligned_labels = []\n",
    "        current_label_index = 0\n",
    "\n",
    "        for offset in offset_mapping:\n",
    "            # If the offset mapping is (0, 0), it's a special token ([CLS], [SEP], [PAD])\n",
    "            if offset == (0, 0):\n",
    "                aligned_labels.append(-100)  # -100 is used to ignore these tokens in the loss computation\n",
    "            else:\n",
    "                # Check if the token is the start of a new word\n",
    "                if offset[0] == 0:\n",
    "                    aligned_labels.append(labels[current_label_index])\n",
    "                    current_label_index += 1\n",
    "                else:\n",
    "                    # If the token is not the first subtoken, you can decide how to label it. \n",
    "                    # For simplicity, let's use the same label as the first subtoken\n",
    "                    aligned_labels.append(labels[current_label_index - 1])\n",
    "\n",
    "        return aligned_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train dataset and test dataset\n",
    "train_dataset = NER_Dataset(train_data, tokenizer)\n",
    "test_dataset = NER_Dataset(test_data, tokenizer)\n",
    "\n",
    "# Create train dataloader and test dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBERT_NER(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (gru): GRU(768, 128, batch_first=True)\n",
       "  (classifier): Linear(in_features=128, out_features=7, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model for NER\n",
    "class MBERT_NER(nn.Module):\n",
    "    def __init__(self, num_labels, gru_hidden_size, num_gru_layers, freeze_bert=False):\n",
    "        super(MBERT_NER, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "        self.gru = nn.GRU(input_size=self.bert.config.hidden_size,\n",
    "                          hidden_size=gru_hidden_size,\n",
    "                          num_layers=num_gru_layers,\n",
    "                          batch_first=True)\n",
    "        self.classifier = nn.Linear(gru_hidden_size, num_labels)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.batch_norm = nn.BatchNorm1d(gru_hidden_size)\n",
    "\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        gru_output, _ = self.gru(sequence_output)\n",
    "        gru_output = self.batch_norm(gru_output)\n",
    "        gru_output = self.dropout(gru_output)\n",
    "        logits = self.classifier(gru_output)\n",
    "        return logits\n",
    "\n",
    "# Create the NER model\n",
    "NUM_LABELS = 7 # Number of NER tags\n",
    "GRU_HIDDEN_SIZE = 128 # Hidden size of the GRU\n",
    "NUM_GRU_LAYERS = 1 # Number of layers in the GRU\n",
    "FREEZE_BERT = False # Whether to freeze the BERT model\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = MBERT_NER(num_labels=NUM_LABELS,\n",
    "                    gru_hidden_size=GRU_HIDDEN_SIZE,\n",
    "                    num_gru_layers=NUM_GRU_LAYERS,\n",
    "                    freeze_bert=FREEZE_BERT)\n",
    "\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6, Train Loss: 1.1648, Train F1: 0.3463, Train Acc: 0.6176, Val Loss: 0.6786, Val F1: 0.6211, Val Acc: 0.7921\n",
      "Epoch 2/6, Train Loss: 0.5805, Train F1: 0.6870, Train Acc: 0.8203, Val Loss: 0.4514, Val F1: 0.7609, Val Acc: 0.8557\n",
      "Epoch 3/6, Train Loss: 0.3804, Train F1: 0.8104, Train Acc: 0.8851, Val Loss: 0.3834, Val F1: 0.8109, Val Acc: 0.8837\n",
      "Epoch 4/6, Train Loss: 0.2604, Train F1: 0.8765, Train Acc: 0.9240, Val Loss: 0.3475, Val F1: 0.8354, Val Acc: 0.8982\n",
      "Epoch 5/6, Train Loss: 0.1818, Train F1: 0.9153, Train Acc: 0.9480, Val Loss: 0.3604, Val F1: 0.8393, Val Acc: 0.9025\n",
      "Epoch 6/6, Train Loss: 0.1526, Train F1: 0.9373, Train Acc: 0.9603, Val Loss: 0.3472, Val F1: 0.8501, Val Acc: 0.9104\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 6\n",
    "\n",
    "train_losses = []\n",
    "train_f1_scores = []\n",
    "train_acc_scores = []\n",
    "val_losses = []\n",
    "val_f1_scores = []\n",
    "val_acc_scores = []\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_predictions, train_labels = [], []\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        logits = logits.view(-1, NUM_LABELS)\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Get predictions and filter out ignored indices for metric calculations\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        active_indices = labels != -100\n",
    "        train_predictions.extend(predictions[active_indices].cpu().numpy())\n",
    "        train_labels.extend(labels[active_indices].cpu().numpy())\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    train_f1 = f1_score(train_labels, train_predictions, average='macro')\n",
    "    train_f1_scores.append(train_f1)\n",
    "\n",
    "    train_acc = accuracy_score(train_labels, train_predictions)\n",
    "    train_acc_scores.append(train_acc)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_predictions, val_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            logits = logits.view(-1, NUM_LABELS)\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Filter predictions and labels\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            active_indices = labels != -100\n",
    "            val_predictions.extend(predictions[active_indices].cpu().numpy())\n",
    "            val_labels.extend(labels[active_indices].cpu().numpy())\n",
    "\n",
    "    val_loss /= len(test_dataloader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    val_f1 = f1_score(val_labels, val_predictions, average='macro')\n",
    "    val_f1_scores.append(val_f1)\n",
    "\n",
    "    val_acc = accuracy_score(val_labels, val_predictions)\n",
    "    val_acc_scores.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: Hindi\n",
      "\n",
      "Weighted F1: 0.7823\n",
      "Macro F1: 0.7217\n",
      "Accuracy: 0.7853\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86      5277\n",
      "           1       0.72      0.84      0.78      1481\n",
      "           2       0.78      0.87      0.82      1874\n",
      "           3       0.60      0.69      0.64       986\n",
      "           4       0.82      0.80      0.81      2602\n",
      "           5       0.61      0.79      0.69      1195\n",
      "           6       0.77      0.32      0.46       867\n",
      "\n",
      "    accuracy                           0.79     14282\n",
      "   macro avg       0.74      0.73      0.72     14282\n",
      "weighted avg       0.80      0.79      0.78     14282\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[4341  262  116  256  121  181    0]\n",
      " [  30 1251  103   49    3   45    0]\n",
      " [  48   68 1639   10   99    0   10]\n",
      " [  53   94    5  682   59   91    2]\n",
      " [ 175   11  130   64 2082   80   60]\n",
      " [  66   57   30   70   21  939   12]\n",
      " [ 130    0   86    9  168  193  281]]\n",
      "--------------------------------------------------\n",
      "Language: Bengali\n",
      "\n",
      "Weighted F1: 0.7439\n",
      "Macro F1: 0.7451\n",
      "Accuracy: 0.7570\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.90      0.79      1835\n",
      "           1       0.77      0.89      0.82      1343\n",
      "           2       0.83      0.87      0.85      2490\n",
      "           3       0.79      0.80      0.79      1239\n",
      "           4       0.78      0.78      0.78      3068\n",
      "           5       0.66      0.77      0.71      1510\n",
      "           6       0.78      0.35      0.48      2080\n",
      "\n",
      "    accuracy                           0.76     13565\n",
      "   macro avg       0.76      0.76      0.75     13565\n",
      "weighted avg       0.76      0.76      0.74     13565\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[1651   25   24   26   60   43    6]\n",
      " [  34 1201   22   22    0   63    1]\n",
      " [ 132   73 2170    7   58    6   44]\n",
      " [  20  108    3  989   13  106    0]\n",
      " [ 207   12  232   65 2378   31  143]\n",
      " [  70  131    9  131    2 1162    5]\n",
      " [ 246   19  166   17  555  359  718]]\n",
      "--------------------------------------------------\n",
      "Language: Marathi\n",
      "\n",
      "Weighted F1: 0.9093\n",
      "Macro F1: 0.8501\n",
      "Accuracy: 0.9104\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96     10408\n",
      "           1       0.86      0.94      0.90      1342\n",
      "           2       0.92      0.95      0.93      1992\n",
      "           3       0.78      0.75      0.76      1172\n",
      "           4       0.89      0.90      0.90      2535\n",
      "           5       0.80      0.85      0.83      1829\n",
      "           6       0.80      0.59      0.68       820\n",
      "\n",
      "    accuracy                           0.91     20098\n",
      "   macro avg       0.86      0.85      0.85     20098\n",
      "weighted avg       0.91      0.91      0.91     20098\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[9943   84   43   85   90  115   48]\n",
      " [  39 1256    8   21    4   14    0]\n",
      " [  21   29 1888    0   38    2   14]\n",
      " [  70   68    0  875   16  143    0]\n",
      " [  55    0   82   43 2291   12   52]\n",
      " [ 130   25    4   97    1 1562   10]\n",
      " [  75    0   29    3  134   97  482]]\n",
      "--------------------------------------------------\n",
      "Language: Tamil\n",
      "\n",
      "Weighted F1: 0.8062\n",
      "Macro F1: 0.6861\n",
      "Accuracy: 0.8084\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91     11461\n",
      "           1       0.71      0.77      0.74      1643\n",
      "           2       0.82      0.78      0.80      2295\n",
      "           3       0.56      0.50      0.53      1512\n",
      "           4       0.71      0.79      0.75      2532\n",
      "           5       0.65      0.79      0.71      1881\n",
      "           6       0.54      0.27      0.36       677\n",
      "\n",
      "    accuracy                           0.81     22001\n",
      "   macro avg       0.70      0.69      0.69     22001\n",
      "weighted avg       0.81      0.81      0.81     22001\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[10284   206   103   330   271   228    39]\n",
      " [   75  1273   131    71    18    68     7]\n",
      " [  158   155  1787    31   113     8    43]\n",
      " [  124    73    18   763   183   348     3]\n",
      " [  265     6    90    66  2008    46    51]\n",
      " [  166    89     7   100    20  1485    14]\n",
      " [  127     0    41     4   220    99   186]]\n",
      "--------------------------------------------------\n",
      "Language: Telugu\n",
      "\n",
      "Weighted F1: 0.8360\n",
      "Macro F1: 0.6641\n",
      "Accuracy: 0.8345\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.93     17291\n",
      "           1       0.61      0.71      0.65      1810\n",
      "           2       0.70      0.85      0.77      1353\n",
      "           3       0.45      0.44      0.45      1375\n",
      "           4       0.76      0.76      0.76      2269\n",
      "           5       0.66      0.84      0.74      2072\n",
      "           6       0.58      0.26      0.36       455\n",
      "\n",
      "    accuracy                           0.83     26625\n",
      "   macro avg       0.67      0.68      0.66     26625\n",
      "weighted avg       0.84      0.83      0.84     26625\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[15592   378   189   360   308   414    50]\n",
      " [  213  1288   123    77    15    94     0]\n",
      " [   36    64  1154     7    75     1    16]\n",
      " [  188   352    10   608    51   162     4]\n",
      " [  135     3   139   190  1720    70    12]\n",
      " [  159    43    14    87    28  1739     2]\n",
      " [   68     0    20    10    70   170   117]]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load the test data from json for all 5 languages\n",
    "languages = ['hindi', 'bengali', 'marathi', 'tamil', 'telugu']\n",
    "\n",
    "# Iterate over all languages and evaluate the model\n",
    "for lang in languages:\n",
    "    with open(f'/home/arnav/BTP/Arnav_Medha/CSE556-NLP-Project/NER-Experiments/Dataset/{lang}_test.json') as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    test_dataset = NER_Dataset(test_data, tokenizer)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    test_predictions, test_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            logits = logits.view(-1, NUM_LABELS)\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            # Filter predictions and labels\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            active_indices = labels != -100\n",
    "            test_predictions.extend(predictions[active_indices].cpu().numpy())\n",
    "            test_labels.extend(labels[active_indices].cpu().numpy())\n",
    "\n",
    "    weighted_f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
    "    macro_f1 = f1_score(test_labels, test_predictions, average='macro')\n",
    "    accuracy = accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "    if lang == 'hindi':\n",
    "        LANG = \"Hindi\"\n",
    "    elif lang == 'bengali':\n",
    "        LANG = \"Bengali\"\n",
    "    elif lang == 'marathi':\n",
    "        LANG = \"Marathi\"\n",
    "    elif lang == 'tamil':\n",
    "        LANG = \"Tamil\"\n",
    "    elif lang == 'telugu':\n",
    "        LANG = \"Telugu\"\n",
    "\n",
    "    print(f\"Language: {LANG}\")\n",
    "    print()\n",
    "    print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "    print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print()\n",
    "    print(\"CLASSIFICATION REPORT: \")\n",
    "    print(classification_report(test_labels, test_predictions))\n",
    "    print()\n",
    "    print(\"CONFUSION MATRIX: \")\n",
    "    print(confusion_matrix(test_labels, test_predictions))\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'NER_FineTune_Marathi.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
