{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import string\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data from json\n",
    "with open('/home/arnav/BTP/Arnav_Medha/CSE556-NLP-Project/NER-Experiments/Dataset/tamil_train.json') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Load test data from json\n",
    "with open('/home/arnav/BTP/Arnav_Medha/CSE556-NLP-Project/NER-Experiments/Dataset/tamil_test.json') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Take the first 1000 samples for training\n",
    "train_data = train_data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokeniser for MBERT\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER_Dataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=128):\n",
    "        self.data = data  # List of sentences and labels\n",
    "        self.tokenizer = tokenizer  # mBERT tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx][\"tokens\"]\n",
    "        word_labels = self.data[idx][\"ner_tags\"]\n",
    "\n",
    "        # Tokenize the text and align the labels\n",
    "        encoding = self.tokenizer(text,\n",
    "                                  is_split_into_words=True,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=self.max_len)\n",
    "\n",
    "        labels = [self.align_labels(encoding['offset_mapping'], word_labels)]\n",
    "\n",
    "        # Remove the offset mapping to prevent issues during model training\n",
    "        del encoding['offset_mapping']\n",
    "\n",
    "        item = {key: torch.tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels[0], dtype=torch.long)\n",
    "\n",
    "        return item\n",
    "    \n",
    "    # Create a function to align labels\n",
    "    def align_labels(self, offset_mapping, labels):\n",
    "        aligned_labels = []\n",
    "        current_label_index = 0\n",
    "\n",
    "        for offset in offset_mapping:\n",
    "            # If the offset mapping is (0, 0), it's a special token ([CLS], [SEP], [PAD])\n",
    "            if offset == (0, 0):\n",
    "                aligned_labels.append(-100)  # -100 is used to ignore these tokens in the loss computation\n",
    "            else:\n",
    "                # Check if the token is the start of a new word\n",
    "                if offset[0] == 0:\n",
    "                    aligned_labels.append(labels[current_label_index])\n",
    "                    current_label_index += 1\n",
    "                else:\n",
    "                    # If the token is not the first subtoken, you can decide how to label it. \n",
    "                    # For simplicity, let's use the same label as the first subtoken\n",
    "                    aligned_labels.append(labels[current_label_index - 1])\n",
    "\n",
    "        return aligned_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train dataset and test dataset\n",
    "train_dataset = NER_Dataset(train_data, tokenizer)\n",
    "test_dataset = NER_Dataset(test_data, tokenizer)\n",
    "\n",
    "# Create train dataloader and test dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBERT_NER(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (gru): GRU(768, 128, batch_first=True)\n",
       "  (classifier): Linear(in_features=128, out_features=7, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model for NER\n",
    "class MBERT_NER(nn.Module):\n",
    "    def __init__(self, num_labels, gru_hidden_size, num_gru_layers, freeze_bert=False):\n",
    "        super(MBERT_NER, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "        self.gru = nn.GRU(input_size=self.bert.config.hidden_size,\n",
    "                          hidden_size=gru_hidden_size,\n",
    "                          num_layers=num_gru_layers,\n",
    "                          batch_first=True)\n",
    "        self.classifier = nn.Linear(gru_hidden_size, num_labels)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.batch_norm = nn.BatchNorm1d(gru_hidden_size)\n",
    "\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        gru_output, _ = self.gru(sequence_output)\n",
    "        gru_output = self.batch_norm(gru_output)\n",
    "        gru_output = self.dropout(gru_output)\n",
    "        logits = self.classifier(gru_output)\n",
    "        return logits\n",
    "\n",
    "# Create the NER model\n",
    "NUM_LABELS = 7 # Number of NER tags\n",
    "GRU_HIDDEN_SIZE = 128 # Hidden size of the GRU\n",
    "NUM_GRU_LAYERS = 1 # Number of layers in the GRU\n",
    "FREEZE_BERT = False # Whether to freeze the BERT model\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = MBERT_NER(num_labels=NUM_LABELS,\n",
    "                    gru_hidden_size=GRU_HIDDEN_SIZE,\n",
    "                    num_gru_layers=NUM_GRU_LAYERS,\n",
    "                    freeze_bert=FREEZE_BERT)\n",
    "\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6, Train Loss: 1.1063, Train F1: 0.3532, Train Acc: 0.6339, Val Loss: 0.7068, Val F1: 0.5864, Val Acc: 0.7803\n",
      "Epoch 2/6, Train Loss: 0.6117, Train F1: 0.6271, Train Acc: 0.8027, Val Loss: 0.5156, Val F1: 0.7066, Val Acc: 0.8386\n",
      "Epoch 3/6, Train Loss: 0.4187, Train F1: 0.7598, Train Acc: 0.8711, Val Loss: 0.4965, Val F1: 0.7499, Val Acc: 0.8511\n",
      "Epoch 4/6, Train Loss: 0.3173, Train F1: 0.8234, Train Acc: 0.9034, Val Loss: 0.4488, Val F1: 0.7757, Val Acc: 0.8624\n",
      "Epoch 5/6, Train Loss: 0.2230, Train F1: 0.8853, Train Acc: 0.9381, Val Loss: 0.4492, Val F1: 0.7912, Val Acc: 0.8751\n",
      "Epoch 6/6, Train Loss: 0.1702, Train F1: 0.9202, Train Acc: 0.9539, Val Loss: 0.4535, Val F1: 0.7982, Val Acc: 0.8792\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 6\n",
    "\n",
    "train_losses = []\n",
    "train_f1_scores = []\n",
    "train_acc_scores = []\n",
    "val_losses = []\n",
    "val_f1_scores = []\n",
    "val_acc_scores = []\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_predictions, train_labels = [], []\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        logits = logits.view(-1, NUM_LABELS)\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Get predictions and filter out ignored indices for metric calculations\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        active_indices = labels != -100\n",
    "        train_predictions.extend(predictions[active_indices].cpu().numpy())\n",
    "        train_labels.extend(labels[active_indices].cpu().numpy())\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    train_f1 = f1_score(train_labels, train_predictions, average='macro')\n",
    "    train_f1_scores.append(train_f1)\n",
    "\n",
    "    train_acc = accuracy_score(train_labels, train_predictions)\n",
    "    train_acc_scores.append(train_acc)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_predictions, val_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            logits = logits.view(-1, NUM_LABELS)\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Filter predictions and labels\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            active_indices = labels != -100\n",
    "            val_predictions.extend(predictions[active_indices].cpu().numpy())\n",
    "            val_labels.extend(labels[active_indices].cpu().numpy())\n",
    "\n",
    "    val_loss /= len(test_dataloader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    val_f1 = f1_score(val_labels, val_predictions, average='macro')\n",
    "    val_f1_scores.append(val_f1)\n",
    "\n",
    "    val_acc = accuracy_score(val_labels, val_predictions)\n",
    "    val_acc_scores.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: Hindi\n",
      "\n",
      "Weighted F1: 0.7801\n",
      "Macro F1: 0.7182\n",
      "Accuracy: 0.7833\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.83      0.86      5277\n",
      "           1       0.70      0.88      0.78      1481\n",
      "           2       0.77      0.94      0.84      1874\n",
      "           3       0.54      0.66      0.59       986\n",
      "           4       0.83      0.75      0.79      2602\n",
      "           5       0.67      0.73      0.70      1195\n",
      "           6       0.74      0.34      0.47       867\n",
      "\n",
      "    accuracy                           0.78     14282\n",
      "   macro avg       0.73      0.73      0.72     14282\n",
      "weighted avg       0.79      0.78      0.78     14282\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[4361  272   65  353   48  157   21]\n",
      " [  23 1307  113   14    7   17    0]\n",
      " [  30   35 1763    8   32    2    4]\n",
      " [  50  131   11  648   86   50   10]\n",
      " [ 243   29  216   72 1941   68   33]\n",
      " [  56   75   20  107   28  872   37]\n",
      " [ 113   16  116    6  193  128  295]]\n",
      "--------------------------------------------------\n",
      "Language: Bengali\n",
      "\n",
      "Weighted F1: 0.6921\n",
      "Macro F1: 0.6914\n",
      "Accuracy: 0.7085\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.94      0.80      1835\n",
      "           1       0.61      0.91      0.73      1343\n",
      "           2       0.65      0.89      0.75      2490\n",
      "           3       0.73      0.69      0.71      1239\n",
      "           4       0.78      0.67      0.72      3068\n",
      "           5       0.75      0.58      0.65      1510\n",
      "           6       0.96      0.32      0.48      2080\n",
      "\n",
      "    accuracy                           0.71     13565\n",
      "   macro avg       0.74      0.71      0.69     13565\n",
      "weighted avg       0.75      0.71      0.69     13565\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[1720   13   21   22   41   15    3]\n",
      " [  37 1222   27   29    0   28    0]\n",
      " [ 148   52 2225    0   60    4    1]\n",
      " [  50  261    0  850   40   38    0]\n",
      " [ 279   42  574   74 2058   24   17]\n",
      " [  66  333   30  191    8  874    8]\n",
      " [ 165   81  571    5  416  180  662]]\n",
      "--------------------------------------------------\n",
      "Language: Marathi\n",
      "\n",
      "Weighted F1: 0.8215\n",
      "Macro F1: 0.7185\n",
      "Accuracy: 0.8272\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93     10408\n",
      "           1       0.69      0.91      0.78      1342\n",
      "           2       0.75      0.87      0.81      1992\n",
      "           3       0.62      0.57      0.60      1172\n",
      "           4       0.80      0.64      0.71      2535\n",
      "           5       0.70      0.75      0.72      1829\n",
      "           6       0.81      0.34      0.48       820\n",
      "\n",
      "    accuracy                           0.83     20098\n",
      "   macro avg       0.76      0.72      0.72     20098\n",
      "weighted avg       0.83      0.83      0.82     20098\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[9729  109   75  132   80  256   27]\n",
      " [  42 1220   16   27    0   37    0]\n",
      " [  84   61 1737    1   36   63   10]\n",
      " [ 111  255   14  673   52   67    0]\n",
      " [ 380   53  372   74 1614   26   16]\n",
      " [ 180   73   11  165   14 1373   13]\n",
      " [  88    1   78    5  228  140  280]]\n",
      "--------------------------------------------------\n",
      "Language: Tamil\n",
      "\n",
      "Weighted F1: 0.8781\n",
      "Macro F1: 0.7982\n",
      "Accuracy: 0.8792\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94     11461\n",
      "           1       0.80      0.85      0.82      1643\n",
      "           2       0.87      0.92      0.90      2295\n",
      "           3       0.76      0.75      0.75      1512\n",
      "           4       0.81      0.86      0.84      2532\n",
      "           5       0.76      0.81      0.79      1881\n",
      "           6       0.72      0.44      0.55       677\n",
      "\n",
      "    accuracy                           0.88     22001\n",
      "   macro avg       0.81      0.80      0.80     22001\n",
      "weighted avg       0.88      0.88      0.88     22001\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[10692   184    85    95   180   200    25]\n",
      " [   66  1396    67    53     9    52     0]\n",
      " [   69    24  2107     1    67     0    27]\n",
      " [   86    98     3  1133    53   137     2]\n",
      " [  110    15   106    38  2186    26    51]\n",
      " [  114    35     8   169    16  1529    10]\n",
      " [   84     2    35     1   184    70   301]]\n",
      "--------------------------------------------------\n",
      "Language: Telugu\n",
      "\n",
      "Weighted F1: 0.8368\n",
      "Macro F1: 0.6820\n",
      "Accuracy: 0.8325\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.89      0.93     17291\n",
      "           1       0.54      0.73      0.62      1810\n",
      "           2       0.63      0.91      0.74      1353\n",
      "           3       0.48      0.40      0.44      1375\n",
      "           4       0.77      0.75      0.76      2269\n",
      "           5       0.69      0.84      0.76      2072\n",
      "           6       0.62      0.47      0.53       455\n",
      "\n",
      "    accuracy                           0.83     26625\n",
      "   macro avg       0.67      0.71      0.68     26625\n",
      "weighted avg       0.85      0.83      0.84     26625\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[15403   639   260   266   256   397    70]\n",
      " [  144  1326   201    73     5    60     1]\n",
      " [   34    29  1228     0    56     0     6]\n",
      " [  139   392    27   553   105   149    10]\n",
      " [  110    16   203   151  1693    66    30]\n",
      " [   95    66    18   117    15  1748    13]\n",
      " [   32     2    24     0    64   120   213]]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load the test data from json for all 5 languages\n",
    "languages = ['hindi', 'bengali', 'marathi', 'tamil', 'telugu']\n",
    "\n",
    "# Iterate over all languages and evaluate the model\n",
    "for lang in languages:\n",
    "    with open(f'/home/arnav/BTP/Arnav_Medha/CSE556-NLP-Project/NER-Experiments/Dataset/{lang}_test.json') as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    test_dataset = NER_Dataset(test_data, tokenizer)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    test_predictions, test_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            logits = logits.view(-1, NUM_LABELS)\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            # Filter predictions and labels\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            active_indices = labels != -100\n",
    "            test_predictions.extend(predictions[active_indices].cpu().numpy())\n",
    "            test_labels.extend(labels[active_indices].cpu().numpy())\n",
    "\n",
    "    weighted_f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
    "    macro_f1 = f1_score(test_labels, test_predictions, average='macro')\n",
    "    accuracy = accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "    if lang == 'hindi':\n",
    "        LANG = \"Hindi\"\n",
    "    elif lang == 'bengali':\n",
    "        LANG = \"Bengali\"\n",
    "    elif lang == 'marathi':\n",
    "        LANG = \"Marathi\"\n",
    "    elif lang == 'tamil':\n",
    "        LANG = \"Tamil\"\n",
    "    elif lang == 'telugu':\n",
    "        LANG = \"Telugu\"\n",
    "\n",
    "    print(f\"Language: {LANG}\")\n",
    "    print()\n",
    "    print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "    print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print()\n",
    "    print(\"CLASSIFICATION REPORT: \")\n",
    "    print(classification_report(test_labels, test_predictions))\n",
    "    print()\n",
    "    print(\"CONFUSION MATRIX: \")\n",
    "    print(confusion_matrix(test_labels, test_predictions))\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'NER_FineTune_Tamil.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
