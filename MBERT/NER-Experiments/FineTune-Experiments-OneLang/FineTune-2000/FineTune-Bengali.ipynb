{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import string\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data from json\n",
    "with open('/home/arnav/BTP/Arnav_Medha/CSE556-NLP-Project/NER-Experiments/Dataset/bengali_train.json') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Load test data from json\n",
    "with open('/home/arnav/BTP/Arnav_Medha/CSE556-NLP-Project/NER-Experiments/Dataset/bengali_test.json') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "train_data = train_data[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokeniser for MBERT\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER_Dataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=128):\n",
    "        self.data = data  # List of sentences and labels\n",
    "        self.tokenizer = tokenizer  # mBERT tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx][\"tokens\"]\n",
    "        word_labels = self.data[idx][\"ner_tags\"]\n",
    "\n",
    "        # Tokenize the text and align the labels\n",
    "        encoding = self.tokenizer(text,\n",
    "                                  is_split_into_words=True,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=self.max_len)\n",
    "\n",
    "        labels = [self.align_labels(encoding['offset_mapping'], word_labels)]\n",
    "\n",
    "        # Remove the offset mapping to prevent issues during model training\n",
    "        del encoding['offset_mapping']\n",
    "\n",
    "        item = {key: torch.tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels[0], dtype=torch.long)\n",
    "\n",
    "        return item\n",
    "    \n",
    "    # Create a function to align labels\n",
    "    def align_labels(self, offset_mapping, labels):\n",
    "        aligned_labels = []\n",
    "        current_label_index = 0\n",
    "\n",
    "        for offset in offset_mapping:\n",
    "            # If the offset mapping is (0, 0), it's a special token ([CLS], [SEP], [PAD])\n",
    "            if offset == (0, 0):\n",
    "                aligned_labels.append(-100)  # -100 is used to ignore these tokens in the loss computation\n",
    "            else:\n",
    "                # Check if the token is the start of a new word\n",
    "                if offset[0] == 0:\n",
    "                    aligned_labels.append(labels[current_label_index])\n",
    "                    current_label_index += 1\n",
    "                else:\n",
    "                    # If the token is not the first subtoken, you can decide how to label it. \n",
    "                    # For simplicity, let's use the same label as the first subtoken\n",
    "                    aligned_labels.append(labels[current_label_index - 1])\n",
    "\n",
    "        return aligned_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train dataset and test dataset\n",
    "train_dataset = NER_Dataset(train_data, tokenizer)\n",
    "test_dataset = NER_Dataset(test_data, tokenizer)\n",
    "\n",
    "# Create train dataloader and test dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBERT_NER(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (gru): GRU(768, 128, batch_first=True)\n",
       "  (classifier): Linear(in_features=128, out_features=7, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model for NER\n",
    "class MBERT_NER(nn.Module):\n",
    "    def __init__(self, num_labels, gru_hidden_size, num_gru_layers, freeze_bert=False):\n",
    "        super(MBERT_NER, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "        self.gru = nn.GRU(input_size=self.bert.config.hidden_size,\n",
    "                          hidden_size=gru_hidden_size,\n",
    "                          num_layers=num_gru_layers,\n",
    "                          batch_first=True)\n",
    "        self.classifier = nn.Linear(gru_hidden_size, num_labels)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.batch_norm = nn.BatchNorm1d(gru_hidden_size)\n",
    "\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        gru_output, _ = self.gru(sequence_output)\n",
    "        gru_output = self.batch_norm(gru_output)\n",
    "        gru_output = self.dropout(gru_output)\n",
    "        logits = self.classifier(gru_output)\n",
    "        return logits\n",
    "\n",
    "# Create the NER model\n",
    "NUM_LABELS = 7 # Number of NER tags\n",
    "GRU_HIDDEN_SIZE = 128 # Hidden size of the GRU\n",
    "NUM_GRU_LAYERS = 1 # Number of layers in the GRU\n",
    "FREEZE_BERT = False # Whether to freeze the BERT model\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = MBERT_NER(num_labels=NUM_LABELS,\n",
    "                    gru_hidden_size=GRU_HIDDEN_SIZE,\n",
    "                    num_gru_layers=NUM_GRU_LAYERS,\n",
    "                    freeze_bert=FREEZE_BERT)\n",
    "\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6, Train Loss: 0.9835, Train F1: 0.6413, Train Acc: 0.6620, Val Loss: 0.5244, Val F1: 0.8277, Val Acc: 0.8328\n",
      "Epoch 2/6, Train Loss: 0.3974, Train F1: 0.8777, Train Acc: 0.8817, Val Loss: 0.3504, Val F1: 0.8929, Val Acc: 0.8955\n",
      "Epoch 3/6, Train Loss: 0.2321, Train F1: 0.9345, Train Acc: 0.9377, Val Loss: 0.3338, Val F1: 0.9072, Val Acc: 0.9078\n",
      "Epoch 4/6, Train Loss: 0.1889, Train F1: 0.9441, Train Acc: 0.9456, Val Loss: 0.3218, Val F1: 0.9247, Val Acc: 0.9250\n",
      "Epoch 5/6, Train Loss: 0.1220, Train F1: 0.9697, Train Acc: 0.9709, Val Loss: 0.2948, Val F1: 0.9268, Val Acc: 0.9279\n",
      "Epoch 6/6, Train Loss: 0.0796, Train F1: 0.9800, Train Acc: 0.9813, Val Loss: 0.3579, Val F1: 0.9224, Val Acc: 0.9223\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 6\n",
    "\n",
    "train_losses = []\n",
    "train_f1_scores = []\n",
    "train_acc_scores = []\n",
    "val_losses = []\n",
    "val_f1_scores = []\n",
    "val_acc_scores = []\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_predictions, train_labels = [], []\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        logits = logits.view(-1, NUM_LABELS)\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Get predictions and filter out ignored indices for metric calculations\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        active_indices = labels != -100\n",
    "        train_predictions.extend(predictions[active_indices].cpu().numpy())\n",
    "        train_labels.extend(labels[active_indices].cpu().numpy())\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    train_f1 = f1_score(train_labels, train_predictions, average='macro')\n",
    "    train_f1_scores.append(train_f1)\n",
    "\n",
    "    train_acc = accuracy_score(train_labels, train_predictions)\n",
    "    train_acc_scores.append(train_acc)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_predictions, val_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            logits = logits.view(-1, NUM_LABELS)\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Filter predictions and labels\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            active_indices = labels != -100\n",
    "            val_predictions.extend(predictions[active_indices].cpu().numpy())\n",
    "            val_labels.extend(labels[active_indices].cpu().numpy())\n",
    "\n",
    "    val_loss /= len(test_dataloader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    val_f1 = f1_score(val_labels, val_predictions, average='macro')\n",
    "    val_f1_scores.append(val_f1)\n",
    "\n",
    "    val_acc = accuracy_score(val_labels, val_predictions)\n",
    "    val_acc_scores.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: Hindi\n",
      "\n",
      "Weighted F1: 0.7370\n",
      "Macro F1: 0.6852\n",
      "Accuracy: 0.7303\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.67      0.79      5277\n",
      "           1       0.71      0.78      0.74      1481\n",
      "           2       0.75      0.90      0.82      1874\n",
      "           3       0.43      0.55      0.48       986\n",
      "           4       0.75      0.82      0.78      2602\n",
      "           5       0.58      0.71      0.64      1195\n",
      "           6       0.48      0.65      0.55       867\n",
      "\n",
      "    accuracy                           0.73     14282\n",
      "   macro avg       0.66      0.72      0.69     14282\n",
      "weighted avg       0.77      0.73      0.74     14282\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[3528  380  158  565  195  266  185]\n",
      " [  20 1148  205   42   24   28   14]\n",
      " [   9   12 1680    3  112    1   57]\n",
      " [  32   54    4  542  161  188    5]\n",
      " [  60    7  135   13 2126   37  224]\n",
      " [  39   12   10  104   53  844  133]\n",
      " [  19    7   42    2  155   80  562]]\n",
      "--------------------------------------------------\n",
      "Language: Bengali\n",
      "\n",
      "Weighted F1: 0.9222\n",
      "Macro F1: 0.9224\n",
      "Accuracy: 0.9223\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.86      0.91      1835\n",
      "           1       0.92      0.94      0.93      1343\n",
      "           2       0.90      0.96      0.93      2490\n",
      "           3       0.91      0.92      0.91      1239\n",
      "           4       0.90      0.93      0.92      3068\n",
      "           5       0.95      0.91      0.93      1510\n",
      "           6       0.95      0.91      0.93      2080\n",
      "\n",
      "    accuracy                           0.92     13565\n",
      "   macro avg       0.93      0.92      0.92     13565\n",
      "weighted avg       0.92      0.92      0.92     13565\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[1583   12   67   15  100   17   41]\n",
      " [  10 1265   13   38    0   17    0]\n",
      " [   7   14 2402    0   56    0   11]\n",
      " [   6   59    0 1135   10   29    0]\n",
      " [  16    0  152    0 2865    0   35]\n",
      " [  17   29    6   55   14 1374   15]\n",
      " [   5    0   42    1  135   10 1887]]\n",
      "--------------------------------------------------\n",
      "Language: Marathi\n",
      "\n",
      "Weighted F1: 0.7402\n",
      "Macro F1: 0.6390\n",
      "Accuracy: 0.7195\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.72      0.82     10408\n",
      "           1       0.75      0.82      0.78      1342\n",
      "           2       0.74      0.87      0.80      1992\n",
      "           3       0.14      0.24      0.17      1172\n",
      "           4       0.65      0.77      0.70      2535\n",
      "           5       0.68      0.75      0.71      1829\n",
      "           6       0.39      0.61      0.48       820\n",
      "\n",
      "    accuracy                           0.72     20098\n",
      "   macro avg       0.61      0.68      0.64     20098\n",
      "weighted avg       0.78      0.72      0.74     20098\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[7502  175  144 1679  376  294  238]\n",
      " [  17 1098  141   19   27   29   11]\n",
      " [  47   49 1739    0   63   61   33]\n",
      " [  51   96   43  280  437  159  106]\n",
      " [  29    6  216   14 1963    8  299]\n",
      " [ 160   38   26   72   62 1376   95]\n",
      " [  62    0   46    0  106  103  503]]\n",
      "--------------------------------------------------\n",
      "Language: Tamil\n",
      "\n",
      "Weighted F1: 0.7510\n",
      "Macro F1: 0.6343\n",
      "Accuracy: 0.7429\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.78      0.86     11461\n",
      "           1       0.68      0.65      0.66      1643\n",
      "           2       0.72      0.87      0.79      2295\n",
      "           3       0.46      0.34      0.39      1512\n",
      "           4       0.57      0.79      0.66      2532\n",
      "           5       0.60      0.77      0.67      1881\n",
      "           6       0.32      0.57      0.41       677\n",
      "\n",
      "    accuracy                           0.74     22001\n",
      "   macro avg       0.61      0.68      0.63     22001\n",
      "weighted avg       0.78      0.74      0.75     22001\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[8956  407  312  410  742  342  292]\n",
      " [  45 1062  341   52   36  105    2]\n",
      " [  60   45 1994    4  134    5   53]\n",
      " [ 106   38   29  511  350  423   55]\n",
      " [ 105    0   69   10 1991   35  322]\n",
      " [ 135    6   16  114   66 1441  103]\n",
      " [  23    0   18    1  196   50  389]]\n",
      "--------------------------------------------------\n",
      "Language: Telugu\n",
      "\n",
      "Weighted F1: 0.7622\n",
      "Macro F1: 0.5653\n",
      "Accuracy: 0.7461\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.79      0.87     17291\n",
      "           1       0.53      0.67      0.59      1810\n",
      "           2       0.49      0.96      0.65      1353\n",
      "           3       0.28      0.20      0.24      1375\n",
      "           4       0.59      0.61      0.60      2269\n",
      "           5       0.58      0.82      0.68      2072\n",
      "           6       0.22      0.65      0.33       455\n",
      "\n",
      "    accuracy                           0.75     26625\n",
      "   macro avg       0.52      0.67      0.57     26625\n",
      "weighted avg       0.81      0.75      0.76     26625\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[13700   742   665   497   648   659   380]\n",
      " [   95  1213   366    70    11    55     0]\n",
      " [   10    14  1295     0    24     0    10]\n",
      " [  119   293    92   278   184   315    94]\n",
      " [   41     1   174    39  1387   139   488]\n",
      " [   99    21    19   103    41  1696    93]\n",
      " [   17     0    16     0    54    71   297]]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load the test data from json for all 5 languages\n",
    "languages = ['hindi', 'bengali', 'marathi', 'tamil', 'telugu']\n",
    "\n",
    "# Iterate over all languages and evaluate the model\n",
    "for lang in languages:\n",
    "    with open(f'/home/arnav/BTP/Arnav_Medha/CSE556-NLP-Project/NER-Experiments/Dataset/{lang}_test.json') as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    test_dataset = NER_Dataset(test_data, tokenizer)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    test_predictions, test_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            logits = logits.view(-1, NUM_LABELS)\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            # Filter predictions and labels\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            active_indices = labels != -100\n",
    "            test_predictions.extend(predictions[active_indices].cpu().numpy())\n",
    "            test_labels.extend(labels[active_indices].cpu().numpy())\n",
    "\n",
    "    weighted_f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
    "    macro_f1 = f1_score(test_labels, test_predictions, average='macro')\n",
    "    accuracy = accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "    if lang == 'hindi':\n",
    "        LANG = \"Hindi\"\n",
    "    elif lang == 'bengali':\n",
    "        LANG = \"Bengali\"\n",
    "    elif lang == 'marathi':\n",
    "        LANG = \"Marathi\"\n",
    "    elif lang == 'tamil':\n",
    "        LANG = \"Tamil\"\n",
    "    elif lang == 'telugu':\n",
    "        LANG = \"Telugu\"\n",
    "\n",
    "    print(f\"Language: {LANG}\")\n",
    "    print()\n",
    "    print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "    print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print()\n",
    "    print(\"CLASSIFICATION REPORT: \")\n",
    "    print(classification_report(test_labels, test_predictions))\n",
    "    print()\n",
    "    print(\"CONFUSION MATRIX: \")\n",
    "    print(confusion_matrix(test_labels, test_predictions))\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'NER_FineTune_Bengali.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
