{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import string\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data from json\n",
    "with open('/home/arnav/BTP/Arnav_Medha/CSE556-NLP-Project/NER-Experiments/Dataset/hindi_train.json') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Load test data from json\n",
    "with open('/home/arnav/BTP/Arnav_Medha/CSE556-NLP-Project/NER-Experiments/Dataset/hindi_test.json') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "train_data = train_data[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokeniser for MBERT\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER_Dataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=128):\n",
    "        self.data = data  # List of sentences and labels\n",
    "        self.tokenizer = tokenizer  # mBERT tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx][\"tokens\"]\n",
    "        word_labels = self.data[idx][\"ner_tags\"]\n",
    "\n",
    "        # Tokenize the text and align the labels\n",
    "        encoding = self.tokenizer(text,\n",
    "                                  is_split_into_words=True,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=self.max_len)\n",
    "\n",
    "        labels = [self.align_labels(encoding['offset_mapping'], word_labels)]\n",
    "\n",
    "        # Remove the offset mapping to prevent issues during model training\n",
    "        del encoding['offset_mapping']\n",
    "\n",
    "        item = {key: torch.tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels[0], dtype=torch.long)\n",
    "\n",
    "        return item\n",
    "    \n",
    "    # Create a function to align labels\n",
    "    def align_labels(self, offset_mapping, labels):\n",
    "        aligned_labels = []\n",
    "        current_label_index = 0\n",
    "\n",
    "        for offset in offset_mapping:\n",
    "            # If the offset mapping is (0, 0), it's a special token ([CLS], [SEP], [PAD])\n",
    "            if offset == (0, 0):\n",
    "                aligned_labels.append(-100)  # -100 is used to ignore these tokens in the loss computation\n",
    "            else:\n",
    "                # Check if the token is the start of a new word\n",
    "                if offset[0] == 0:\n",
    "                    aligned_labels.append(labels[current_label_index])\n",
    "                    current_label_index += 1\n",
    "                else:\n",
    "                    # If the token is not the first subtoken, you can decide how to label it. \n",
    "                    # For simplicity, let's use the same label as the first subtoken\n",
    "                    aligned_labels.append(labels[current_label_index - 1])\n",
    "\n",
    "        return aligned_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train dataset and test dataset\n",
    "train_dataset = NER_Dataset(train_data, tokenizer)\n",
    "test_dataset = NER_Dataset(test_data, tokenizer)\n",
    "\n",
    "# Create train dataloader and test dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBERT_NER(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (gru): GRU(768, 128, batch_first=True)\n",
       "  (classifier): Linear(in_features=128, out_features=7, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model for NER\n",
    "class MBERT_NER(nn.Module):\n",
    "    def __init__(self, num_labels, gru_hidden_size, num_gru_layers, freeze_bert=False):\n",
    "        super(MBERT_NER, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "        self.gru = nn.GRU(input_size=self.bert.config.hidden_size,\n",
    "                          hidden_size=gru_hidden_size,\n",
    "                          num_layers=num_gru_layers,\n",
    "                          batch_first=True)\n",
    "        self.classifier = nn.Linear(gru_hidden_size, num_labels)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.batch_norm = nn.BatchNorm1d(gru_hidden_size)\n",
    "\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        gru_output, _ = self.gru(sequence_output)\n",
    "        gru_output = self.batch_norm(gru_output)\n",
    "        gru_output = self.dropout(gru_output)\n",
    "        logits = self.classifier(gru_output)\n",
    "        return logits\n",
    "\n",
    "# Create the NER model\n",
    "NUM_LABELS = 7 # Number of NER tags\n",
    "GRU_HIDDEN_SIZE = 128 # Hidden size of the GRU\n",
    "NUM_GRU_LAYERS = 1 # Number of layers in the GRU\n",
    "FREEZE_BERT = False # Whether to freeze the BERT model\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = MBERT_NER(num_labels=NUM_LABELS,\n",
    "                    gru_hidden_size=GRU_HIDDEN_SIZE,\n",
    "                    num_gru_layers=NUM_GRU_LAYERS,\n",
    "                    freeze_bert=FREEZE_BERT)\n",
    "\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6, Train Loss: 0.9247, Train F1: 0.5924, Train Acc: 0.6986, Val Loss: 0.5537, Val F1: 0.7633, Val Acc: 0.8270\n",
      "Epoch 2/6, Train Loss: 0.4129, Train F1: 0.8235, Train Acc: 0.8753, Val Loss: 0.3731, Val F1: 0.8340, Val Acc: 0.8843\n",
      "Epoch 3/6, Train Loss: 0.2700, Train F1: 0.8886, Train Acc: 0.9215, Val Loss: 0.3556, Val F1: 0.8470, Val Acc: 0.8967\n",
      "Epoch 4/6, Train Loss: 0.1938, Train F1: 0.9196, Train Acc: 0.9437, Val Loss: 0.3566, Val F1: 0.8665, Val Acc: 0.9017\n",
      "Epoch 5/6, Train Loss: 0.1648, Train F1: 0.9344, Train Acc: 0.9536, Val Loss: 0.3391, Val F1: 0.8686, Val Acc: 0.9056\n",
      "Epoch 6/6, Train Loss: 0.1075, Train F1: 0.9611, Train Acc: 0.9719, Val Loss: 0.3297, Val F1: 0.8883, Val Acc: 0.9164\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 6\n",
    "\n",
    "train_losses = []\n",
    "train_f1_scores = []\n",
    "train_acc_scores = []\n",
    "val_losses = []\n",
    "val_f1_scores = []\n",
    "val_acc_scores = []\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_predictions, train_labels = [], []\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        logits = logits.view(-1, NUM_LABELS)\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Get predictions and filter out ignored indices for metric calculations\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        active_indices = labels != -100\n",
    "        train_predictions.extend(predictions[active_indices].cpu().numpy())\n",
    "        train_labels.extend(labels[active_indices].cpu().numpy())\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    train_f1 = f1_score(train_labels, train_predictions, average='macro')\n",
    "    train_f1_scores.append(train_f1)\n",
    "\n",
    "    train_acc = accuracy_score(train_labels, train_predictions)\n",
    "    train_acc_scores.append(train_acc)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_predictions, val_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            logits = logits.view(-1, NUM_LABELS)\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Filter predictions and labels\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            active_indices = labels != -100\n",
    "            val_predictions.extend(predictions[active_indices].cpu().numpy())\n",
    "            val_labels.extend(labels[active_indices].cpu().numpy())\n",
    "\n",
    "    val_loss /= len(test_dataloader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    val_f1 = f1_score(val_labels, val_predictions, average='macro')\n",
    "    val_f1_scores.append(val_f1)\n",
    "\n",
    "    val_acc = accuracy_score(val_labels, val_predictions)\n",
    "    val_acc_scores.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: Hindi\n",
      "\n",
      "Weighted F1: 0.9155\n",
      "Macro F1: 0.8883\n",
      "Accuracy: 0.9164\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96      5277\n",
      "           1       0.90      0.92      0.91      1481\n",
      "           2       0.92      0.94      0.93      1874\n",
      "           3       0.83      0.88      0.85       986\n",
      "           4       0.92      0.93      0.92      2602\n",
      "           5       0.88      0.81      0.84      1195\n",
      "           6       0.90      0.73      0.81       867\n",
      "\n",
      "    accuracy                           0.92     14282\n",
      "   macro avg       0.90      0.88      0.89     14282\n",
      "weighted avg       0.92      0.92      0.92     14282\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[5082   61   30   22   31   39   12]\n",
      " [  45 1367   10   47    0   12    0]\n",
      " [  25   16 1758    1   67    3    4]\n",
      " [  22   47    2  867    8   39    1]\n",
      " [  64    3   82    0 2412   12   29]\n",
      " [  52   28    4  108   12  966   25]\n",
      " [  71    0   34    0   94   32  636]]\n",
      "--------------------------------------------------\n",
      "Language: Bengali\n",
      "\n",
      "Weighted F1: 0.7602\n",
      "Macro F1: 0.7576\n",
      "Accuracy: 0.7669\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.95      0.82      1835\n",
      "           1       0.76      0.85      0.80      1343\n",
      "           2       0.87      0.79      0.83      2490\n",
      "           3       0.69      0.85      0.76      1239\n",
      "           4       0.74      0.84      0.78      3068\n",
      "           5       0.84      0.61      0.70      1510\n",
      "           6       0.81      0.49      0.61      2080\n",
      "\n",
      "    accuracy                           0.77     13565\n",
      "   macro avg       0.77      0.77      0.76     13565\n",
      "weighted avg       0.78      0.77      0.76     13565\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[1749   11    8   17   36   14    0]\n",
      " [  67 1138   11  109    0   18    0]\n",
      " [ 211   29 1967    0  201    0   82]\n",
      " [  17  102    0 1049   19   52    0]\n",
      " [ 129   12  167   41 2565   11  143]\n",
      " [  91  187    0  300    9  916    7]\n",
      " [ 185   11  117    9  655   84 1019]]\n",
      "--------------------------------------------------\n",
      "Language: Marathi\n",
      "\n",
      "Weighted F1: 0.8353\n",
      "Macro F1: 0.7491\n",
      "Accuracy: 0.8391\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92     10408\n",
      "           1       0.77      0.88      0.82      1342\n",
      "           2       0.83      0.83      0.83      1992\n",
      "           3       0.61      0.65      0.63      1172\n",
      "           4       0.81      0.69      0.75      2535\n",
      "           5       0.78      0.70      0.74      1829\n",
      "           6       0.69      0.47      0.56       820\n",
      "\n",
      "    accuracy                           0.84     20098\n",
      "   macro avg       0.77      0.74      0.75     20098\n",
      "weighted avg       0.84      0.84      0.84     20098\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[9858   65   23  158   83  153   68]\n",
      " [  61 1176   21   74    1    9    0]\n",
      " [ 117   60 1653    3   84   62   13]\n",
      " [ 113  187    8  759   34   66    5]\n",
      " [ 398    3  250   50 1753   14   67]\n",
      " [ 273   33   15  200   10 1280   18]\n",
      " [ 144    0   22    5  206   57  386]]\n",
      "--------------------------------------------------\n",
      "Language: Tamil\n",
      "\n",
      "Weighted F1: 0.8006\n",
      "Macro F1: 0.7027\n",
      "Accuracy: 0.7998\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89     11461\n",
      "           1       0.74      0.68      0.71      1643\n",
      "           2       0.84      0.79      0.82      2295\n",
      "           3       0.52      0.55      0.53      1512\n",
      "           4       0.68      0.77      0.72      2532\n",
      "           5       0.79      0.72      0.75      1881\n",
      "           6       0.49      0.50      0.49       677\n",
      "\n",
      "    accuracy                           0.80     22001\n",
      "   macro avg       0.71      0.70      0.70     22001\n",
      "weighted avg       0.80      0.80      0.80     22001\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[10183   256    40   501   228   124   129]\n",
      " [  139  1113   193    87    42    58    11]\n",
      " [  249    53  1823    10   113     0    47]\n",
      " [  157    67     9   829   334   108     8]\n",
      " [  328    13    77    17  1954    25   118]\n",
      " [  267    11     6   144    51  1353    49]\n",
      " [  138     0    16     1   143    38   341]]\n",
      "--------------------------------------------------\n",
      "Language: Telugu\n",
      "\n",
      "Weighted F1: 0.8284\n",
      "Macro F1: 0.6710\n",
      "Accuracy: 0.8231\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.89      0.92     17291\n",
      "           1       0.61      0.64      0.62      1810\n",
      "           2       0.67      0.93      0.78      1353\n",
      "           3       0.31      0.34      0.32      1375\n",
      "           4       0.73      0.72      0.72      2269\n",
      "           5       0.78      0.81      0.79      2072\n",
      "           6       0.46      0.65      0.54       455\n",
      "\n",
      "    accuracy                           0.82     26625\n",
      "   macro avg       0.64      0.71      0.67     26625\n",
      "weighted avg       0.84      0.82      0.83     26625\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[15428   412   152   717   257   218   107]\n",
      " [  193  1163   262   123    15    52     2]\n",
      " [   35    20  1260     0    35     0     3]\n",
      " [  210   283    37   470   194   143    38]\n",
      " [  186     5   160   128  1629    34   127]\n",
      " [  153    34     2   102    39  1670    72]\n",
      " [   62     0    21     0    57    20   295]]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load the test data from json for all 5 languages\n",
    "languages = ['hindi', 'bengali', 'marathi', 'tamil', 'telugu']\n",
    "\n",
    "# Iterate over all languages and evaluate the model\n",
    "for lang in languages:\n",
    "    with open(f'/home/arnav/BTP/Arnav_Medha/CSE556-NLP-Project/NER-Experiments/Dataset/{lang}_test.json') as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    test_dataset = NER_Dataset(test_data, tokenizer)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    test_predictions, test_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            logits = logits.view(-1, NUM_LABELS)\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            # Filter predictions and labels\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            active_indices = labels != -100\n",
    "            test_predictions.extend(predictions[active_indices].cpu().numpy())\n",
    "            test_labels.extend(labels[active_indices].cpu().numpy())\n",
    "\n",
    "    weighted_f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
    "    macro_f1 = f1_score(test_labels, test_predictions, average='macro')\n",
    "    accuracy = accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "    if lang == 'hindi':\n",
    "        LANG = \"Hindi\"\n",
    "    elif lang == 'bengali':\n",
    "        LANG = \"Bengali\"\n",
    "    elif lang == 'marathi':\n",
    "        LANG = \"Marathi\"\n",
    "    elif lang == 'tamil':\n",
    "        LANG = \"Tamil\"\n",
    "    elif lang == 'telugu':\n",
    "        LANG = \"Telugu\"\n",
    "\n",
    "    print(f\"Language: {LANG}\")\n",
    "    print()\n",
    "    print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "    print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print()\n",
    "    print(\"CLASSIFICATION REPORT: \")\n",
    "    print(classification_report(test_labels, test_predictions))\n",
    "    print()\n",
    "    print(\"CONFUSION MATRIX: \")\n",
    "    print(confusion_matrix(test_labels, test_predictions))\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'NER_FineTune_Hi.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
