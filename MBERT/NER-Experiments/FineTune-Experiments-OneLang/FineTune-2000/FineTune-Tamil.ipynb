{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import string\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data from json\n",
    "with open('/home/arnav/BTP/Arnav_Medha/CSE556-NLP-Project/NER-Experiments/Dataset/tamil_train.json') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Load test data from json\n",
    "with open('/home/arnav/BTP/Arnav_Medha/CSE556-NLP-Project/NER-Experiments/Dataset/tamil_test.json') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "train_data = train_data[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokeniser for MBERT\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER_Dataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=128):\n",
    "        self.data = data  # List of sentences and labels\n",
    "        self.tokenizer = tokenizer  # mBERT tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx][\"tokens\"]\n",
    "        word_labels = self.data[idx][\"ner_tags\"]\n",
    "\n",
    "        # Tokenize the text and align the labels\n",
    "        encoding = self.tokenizer(text,\n",
    "                                  is_split_into_words=True,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=self.max_len)\n",
    "\n",
    "        labels = [self.align_labels(encoding['offset_mapping'], word_labels)]\n",
    "\n",
    "        # Remove the offset mapping to prevent issues during model training\n",
    "        del encoding['offset_mapping']\n",
    "\n",
    "        item = {key: torch.tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels[0], dtype=torch.long)\n",
    "\n",
    "        return item\n",
    "    \n",
    "    # Create a function to align labels\n",
    "    def align_labels(self, offset_mapping, labels):\n",
    "        aligned_labels = []\n",
    "        current_label_index = 0\n",
    "\n",
    "        for offset in offset_mapping:\n",
    "            # If the offset mapping is (0, 0), it's a special token ([CLS], [SEP], [PAD])\n",
    "            if offset == (0, 0):\n",
    "                aligned_labels.append(-100)  # -100 is used to ignore these tokens in the loss computation\n",
    "            else:\n",
    "                # Check if the token is the start of a new word\n",
    "                if offset[0] == 0:\n",
    "                    aligned_labels.append(labels[current_label_index])\n",
    "                    current_label_index += 1\n",
    "                else:\n",
    "                    # If the token is not the first subtoken, you can decide how to label it. \n",
    "                    # For simplicity, let's use the same label as the first subtoken\n",
    "                    aligned_labels.append(labels[current_label_index - 1])\n",
    "\n",
    "        return aligned_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train dataset and test dataset\n",
    "train_dataset = NER_Dataset(train_data, tokenizer)\n",
    "test_dataset = NER_Dataset(test_data, tokenizer)\n",
    "\n",
    "# Create train dataloader and test dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBERT_NER(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (gru): GRU(768, 128, batch_first=True)\n",
       "  (classifier): Linear(in_features=128, out_features=7, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model for NER\n",
    "class MBERT_NER(nn.Module):\n",
    "    def __init__(self, num_labels, gru_hidden_size, num_gru_layers, freeze_bert=False):\n",
    "        super(MBERT_NER, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "        self.gru = nn.GRU(input_size=self.bert.config.hidden_size,\n",
    "                          hidden_size=gru_hidden_size,\n",
    "                          num_layers=num_gru_layers,\n",
    "                          batch_first=True)\n",
    "        self.classifier = nn.Linear(gru_hidden_size, num_labels)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.batch_norm = nn.BatchNorm1d(gru_hidden_size)\n",
    "\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        gru_output, _ = self.gru(sequence_output)\n",
    "        gru_output = self.batch_norm(gru_output)\n",
    "        gru_output = self.dropout(gru_output)\n",
    "        logits = self.classifier(gru_output)\n",
    "        return logits\n",
    "\n",
    "# Create the NER model\n",
    "NUM_LABELS = 7 # Number of NER tags\n",
    "GRU_HIDDEN_SIZE = 128 # Hidden size of the GRU\n",
    "NUM_GRU_LAYERS = 1 # Number of layers in the GRU\n",
    "FREEZE_BERT = False # Whether to freeze the BERT model\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = MBERT_NER(num_labels=NUM_LABELS,\n",
    "                    gru_hidden_size=GRU_HIDDEN_SIZE,\n",
    "                    num_gru_layers=NUM_GRU_LAYERS,\n",
    "                    freeze_bert=FREEZE_BERT)\n",
    "\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6, Train Loss: 0.9276, Train F1: 0.4863, Train Acc: 0.6936, Val Loss: 0.5215, Val F1: 0.7326, Val Acc: 0.8404\n",
      "Epoch 2/6, Train Loss: 0.4485, Train F1: 0.7643, Train Acc: 0.8614, Val Loss: 0.4210, Val F1: 0.7982, Val Acc: 0.8712\n",
      "Epoch 3/6, Train Loss: 0.3092, Train F1: 0.8427, Train Acc: 0.9078, Val Loss: 0.4107, Val F1: 0.7957, Val Acc: 0.8836\n",
      "Epoch 4/6, Train Loss: 0.2283, Train F1: 0.8903, Train Acc: 0.9357, Val Loss: 0.3907, Val F1: 0.8250, Val Acc: 0.8915\n",
      "Epoch 5/6, Train Loss: 0.1695, Train F1: 0.9224, Train Acc: 0.9545, Val Loss: 0.4049, Val F1: 0.8408, Val Acc: 0.8981\n",
      "Epoch 6/6, Train Loss: 0.1219, Train F1: 0.9450, Train Acc: 0.9678, Val Loss: 0.4129, Val F1: 0.8363, Val Acc: 0.8986\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 6\n",
    "\n",
    "train_losses = []\n",
    "train_f1_scores = []\n",
    "train_acc_scores = []\n",
    "val_losses = []\n",
    "val_f1_scores = []\n",
    "val_acc_scores = []\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_predictions, train_labels = [], []\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        logits = logits.view(-1, NUM_LABELS)\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Get predictions and filter out ignored indices for metric calculations\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        active_indices = labels != -100\n",
    "        train_predictions.extend(predictions[active_indices].cpu().numpy())\n",
    "        train_labels.extend(labels[active_indices].cpu().numpy())\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    train_f1 = f1_score(train_labels, train_predictions, average='macro')\n",
    "    train_f1_scores.append(train_f1)\n",
    "\n",
    "    train_acc = accuracy_score(train_labels, train_predictions)\n",
    "    train_acc_scores.append(train_acc)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_predictions, val_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            logits = logits.view(-1, NUM_LABELS)\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Filter predictions and labels\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            active_indices = labels != -100\n",
    "            val_predictions.extend(predictions[active_indices].cpu().numpy())\n",
    "            val_labels.extend(labels[active_indices].cpu().numpy())\n",
    "\n",
    "    val_loss /= len(test_dataloader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    val_f1 = f1_score(val_labels, val_predictions, average='macro')\n",
    "    val_f1_scores.append(val_f1)\n",
    "\n",
    "    val_acc = accuracy_score(val_labels, val_predictions)\n",
    "    val_acc_scores.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: Hindi\n",
      "\n",
      "Weighted F1: 0.8057\n",
      "Macro F1: 0.7513\n",
      "Accuracy: 0.8035\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.85      0.88      5277\n",
      "           1       0.80      0.82      0.81      1481\n",
      "           2       0.83      0.88      0.86      1874\n",
      "           3       0.58      0.68      0.62       986\n",
      "           4       0.87      0.76      0.81      2602\n",
      "           5       0.64      0.83      0.72      1195\n",
      "           6       0.59      0.54      0.56       867\n",
      "\n",
      "    accuracy                           0.80     14282\n",
      "   macro avg       0.74      0.77      0.75     14282\n",
      "weighted avg       0.81      0.80      0.81     14282\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[4511  159   22  309   33  207   36]\n",
      " [  40 1215  129   25    3   57   12]\n",
      " [  58   29 1648    9   52    5   73]\n",
      " [  55   76    0  667   62  107   19]\n",
      " [ 228   10  123   72 1975   66  128]\n",
      " [  26   25    4   68   20  988   64]\n",
      " [  77    9   49    8  133  120  471]]\n",
      "--------------------------------------------------\n",
      "Language: Bengali\n",
      "\n",
      "Weighted F1: 0.7737\n",
      "Macro F1: 0.7751\n",
      "Accuracy: 0.7791\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.94      0.82      1835\n",
      "           1       0.75      0.86      0.80      1343\n",
      "           2       0.75      0.86      0.80      2490\n",
      "           3       0.80      0.81      0.80      1239\n",
      "           4       0.85      0.75      0.80      3068\n",
      "           5       0.77      0.78      0.77      1510\n",
      "           6       0.81      0.51      0.63      2080\n",
      "\n",
      "    accuracy                           0.78     13565\n",
      "   macro avg       0.78      0.79      0.78     13565\n",
      "weighted avg       0.79      0.78      0.77     13565\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[1724   10    5   24   22   33   17]\n",
      " [  87 1157   28   30    0   39    2]\n",
      " [ 215   26 2151    3   53    5   37]\n",
      " [  31  123    0 1002    6   74    3]\n",
      " [ 173    9  312   70 2297   29  178]\n",
      " [  22  197    0  104    1 1175   11]\n",
      " [ 125   17  369   24  309  174 1062]]\n",
      "--------------------------------------------------\n",
      "Language: Marathi\n",
      "\n",
      "Weighted F1: 0.8212\n",
      "Macro F1: 0.7212\n",
      "Accuracy: 0.8250\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92     10408\n",
      "           1       0.76      0.87      0.81      1342\n",
      "           2       0.81      0.82      0.81      1992\n",
      "           3       0.60      0.53      0.56      1172\n",
      "           4       0.83      0.64      0.72      2535\n",
      "           5       0.66      0.82      0.73      1829\n",
      "           6       0.59      0.42      0.49       820\n",
      "\n",
      "    accuracy                           0.82     20098\n",
      "   macro avg       0.74      0.72      0.72     20098\n",
      "weighted avg       0.82      0.82      0.82     20098\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[9696   65   25  190   69  281   82]\n",
      " [  51 1165   20   26    0   80    0]\n",
      " [ 108   52 1627   10   34   88   73]\n",
      " [  85  201   10  622   47  199    8]\n",
      " [ 451   18  282   73 1618   21   72]\n",
      " [ 148   38    4  115   10 1506    8]\n",
      " [ 146    0   36    2  168  122  346]]\n",
      "--------------------------------------------------\n",
      "Language: Tamil\n",
      "\n",
      "Weighted F1: 0.8996\n",
      "Macro F1: 0.8363\n",
      "Accuracy: 0.8986\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96     11461\n",
      "           1       0.86      0.85      0.86      1643\n",
      "           2       0.93      0.88      0.90      2295\n",
      "           3       0.82      0.76      0.79      1512\n",
      "           4       0.89      0.84      0.86      2532\n",
      "           5       0.74      0.90      0.81      1881\n",
      "           6       0.63      0.72      0.67       677\n",
      "\n",
      "    accuracy                           0.90     22001\n",
      "   macro avg       0.83      0.84      0.84     22001\n",
      "weighted avg       0.90      0.90      0.90     22001\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[10899    97    31    57   106   217    54]\n",
      " [   57  1398    43    59     4    78     4]\n",
      " [  101    25  2023    14    57     1    74]\n",
      " [   48    77     3  1145    27   212     0]\n",
      " [  112     7    69    23  2132    43   146]\n",
      " [   73    15     0    91     8  1684    10]\n",
      " [   71     0    10     3    75    28   490]]\n",
      "--------------------------------------------------\n",
      "Language: Telugu\n",
      "\n",
      "Weighted F1: 0.8610\n",
      "Macro F1: 0.7191\n",
      "Accuracy: 0.8602\n",
      "\n",
      "CLASSIFICATION REPORT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.94     17291\n",
      "           1       0.69      0.77      0.73      1810\n",
      "           2       0.81      0.87      0.84      1353\n",
      "           3       0.52      0.39      0.44      1375\n",
      "           4       0.86      0.70      0.77      2269\n",
      "           5       0.64      0.91      0.75      2072\n",
      "           6       0.49      0.67      0.56       455\n",
      "\n",
      "    accuracy                           0.86     26625\n",
      "   macro avg       0.71      0.75      0.72     26625\n",
      "weighted avg       0.87      0.86      0.86     26625\n",
      "\n",
      "\n",
      "CONFUSION MATRIX: \n",
      "[[16025   216    46   184   163   522   135]\n",
      " [  172  1397    68    64     7   102     0]\n",
      " [   74    42  1172     0    33     0    32]\n",
      " [  164   349    10   533    26   272    21]\n",
      " [  158     3   140   187  1585    81   115]\n",
      " [   74    24     0    60     8  1888    18]\n",
      " [   30     0    17     0    29    76   303]]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load the test data from json for all 5 languages\n",
    "languages = ['hindi', 'bengali', 'marathi', 'tamil', 'telugu']\n",
    "\n",
    "# Iterate over all languages and evaluate the model\n",
    "for lang in languages:\n",
    "    with open(f'/home/arnav/BTP/Arnav_Medha/CSE556-NLP-Project/NER-Experiments/Dataset/{lang}_test.json') as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    test_dataset = NER_Dataset(test_data, tokenizer)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    test_predictions, test_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            logits = logits.view(-1, NUM_LABELS)\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            # Filter predictions and labels\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            active_indices = labels != -100\n",
    "            test_predictions.extend(predictions[active_indices].cpu().numpy())\n",
    "            test_labels.extend(labels[active_indices].cpu().numpy())\n",
    "\n",
    "    weighted_f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
    "    macro_f1 = f1_score(test_labels, test_predictions, average='macro')\n",
    "    accuracy = accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "    if lang == 'hindi':\n",
    "        LANG = \"Hindi\"\n",
    "    elif lang == 'bengali':\n",
    "        LANG = \"Bengali\"\n",
    "    elif lang == 'marathi':\n",
    "        LANG = \"Marathi\"\n",
    "    elif lang == 'tamil':\n",
    "        LANG = \"Tamil\"\n",
    "    elif lang == 'telugu':\n",
    "        LANG = \"Telugu\"\n",
    "\n",
    "    print(f\"Language: {LANG}\")\n",
    "    print()\n",
    "    print(f\"Weighted F1: {weighted_f1:.4f}\")\n",
    "    print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print()\n",
    "    print(\"CLASSIFICATION REPORT: \")\n",
    "    print(classification_report(test_labels, test_predictions))\n",
    "    print()\n",
    "    print(\"CONFUSION MATRIX: \")\n",
    "    print(confusion_matrix(test_labels, test_predictions))\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'NER_FineTune_Tamil.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
