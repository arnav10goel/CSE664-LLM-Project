{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8220366,"sourceType":"datasetVersion","datasetId":4873271}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"- Importing Libraries","metadata":{}},{"cell_type":"code","source":"# !git clone https://github.com/arnav10goel/CSE556-NLP-Project.git\n# %cd CSE556-NLP-Project","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T16:06:22.603338Z","iopub.execute_input":"2024-11-17T16:06:22.604142Z","iopub.status.idle":"2024-11-17T16:06:22.608025Z","shell.execute_reply.started":"2024-11-17T16:06:22.604101Z","shell.execute_reply":"2024-11-17T16:06:22.607025Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import torch\nfrom transformers import BertModel\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport torch\nfrom transformers import BertTokenizer\nimport string","metadata":{"execution":{"iopub.status.busy":"2024-11-17T16:06:22.609701Z","iopub.execute_input":"2024-11-17T16:06:22.610014Z","iopub.status.idle":"2024-11-17T16:06:22.620435Z","shell.execute_reply.started":"2024-11-17T16:06:22.609979Z","shell.execute_reply":"2024-11-17T16:06:22.619637Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"### Load Dataset for the Particular Language\n- Here insert the language initial you want to load the train and test CSVs for\n- Language Codes:\n    1. Hindi: `hi`\n    2. Bengali: `bn`\n    3. Marathi: `mr`\n    4. Tamil: `ta`\n    5. Telugu: `te`","metadata":{}},{"cell_type":"code","source":"lang = 'Bengali'\n\ntrain_file = f\"/kaggle/working/CSE556-NLP-Project/Hate-Speech-Detection-Experiments/Dataset/{lang}_train.csv\"\n\ntest_file = f\"/kaggle/working/CSE556-NLP-Project/Hate-Speech-Detection-Experiments/Dataset/{lang}_test.csv\"\n\ntrain_df = pd.read_csv(train_file)\ntest_df = pd.read_csv(test_file)","metadata":{"execution":{"iopub.status.busy":"2024-11-17T16:06:22.622060Z","iopub.execute_input":"2024-11-17T16:06:22.622777Z","iopub.status.idle":"2024-11-17T16:06:22.683448Z","shell.execute_reply.started":"2024-11-17T16:06:22.622734Z","shell.execute_reply":"2024-11-17T16:06:22.682659Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2024-11-17T16:06:22.684528Z","iopub.execute_input":"2024-11-17T16:06:22.684865Z","iopub.status.idle":"2024-11-17T16:06:22.698969Z","shell.execute_reply.started":"2024-11-17T16:06:22.684832Z","shell.execute_reply":"2024-11-17T16:06:22.697991Z"},"trusted":true},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"                                                   Post  Labels Set  \\\n0     শুধু সাব্বির বেয়াদপ না পুরো ক্রিকেট টিমটাই বেয়...           1   \n1                      বাংলার মাটির মানুষ মিজানুর রহমান           0   \n2                     টিনের চালে কাওয়া, তুমি আমার শাওয়া           1   \n3     জাজদের কে কে ঘন্টা হিসেবে কানে গলায় থাপরাতে চা...           0   \n4                                                বেয়াদব           0   \n...                                                 ...         ...   \n3995     দেওয়ানবাগী হচ্ছে এক্সট্রিম লেভেল এর মাদারচোদ।।           1   \n3996  মাদারচদদের কেউ ব্যবহার শেখা।গাজা খাইয়া জাজ হইছ...           1   \n3997                  এই মাদারচোদেরা কি তদন্তের কথা বলে           1   \n3998  ওসি মোয়াজ্জিম যদি রেকডিং না করলে আমরা নুসরাতের...           0   \n3999  আল্লাহ বাচাইচে কুন দাড়ি ওয়ালা নাই একটা দাড়িওয়া...           0   \n\n                     Category               Dataset  \\\n0                      sports  Bengali hate speech    \n1                    religion  Bengali hate speech    \n2                   celebrity  Bengali hate speech    \n3               entertainment  Bengali hate speech    \n4     Meme, TikTok and others  Bengali hate speech    \n...                       ...                   ...   \n3995                 religion  Bengali hate speech    \n3996            entertainment  Bengali hate speech    \n3997                    crime  Bengali hate speech    \n3998                    crime  Bengali hate speech    \n3999                    crime  Bengali hate speech    \n\n                                         Processed_Post  \n0     শুধু সাব্বির বেয়াদপ না পুরো ক্রিকেট টিমটাই বেয়...  \n1                      বাংলার মাটির মানুষ মিজানুর রহমান  \n2                     টিনের চালে কাওয়া, তুমি আমার শাওয়া  \n3     জাজদের কে কে ঘন্টা হিসেবে কানে গলায় থাপরাতে চা...  \n4                                                বেয়াদব  \n...                                                 ...  \n3995     দেওয়ানবাগী হচ্ছে এক্সট্রিম লেভেল এর মাদারচোদ।।  \n3996  মাদারচদদের কেউ ব্যবহার শেখা।গাজা খাইয়া জাজ হইছ...  \n3997                  এই মাদারচোদেরা কি তদন্তের কথা বলে  \n3998  ওসি মোয়াজ্জিম যদি রেকডিং না করলে আমরা নুসরাতের...  \n3999  আল্লাহ বাচাইচে কুন দাড়ি ওয়ালা নাই একটা দাড়িওয়া...  \n\n[4000 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Post</th>\n      <th>Labels Set</th>\n      <th>Category</th>\n      <th>Dataset</th>\n      <th>Processed_Post</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>শুধু সাব্বির বেয়াদপ না পুরো ক্রিকেট টিমটাই বেয়...</td>\n      <td>1</td>\n      <td>sports</td>\n      <td>Bengali hate speech</td>\n      <td>শুধু সাব্বির বেয়াদপ না পুরো ক্রিকেট টিমটাই বেয়...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>বাংলার মাটির মানুষ মিজানুর রহমান</td>\n      <td>0</td>\n      <td>religion</td>\n      <td>Bengali hate speech</td>\n      <td>বাংলার মাটির মানুষ মিজানুর রহমান</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>টিনের চালে কাওয়া, তুমি আমার শাওয়া</td>\n      <td>1</td>\n      <td>celebrity</td>\n      <td>Bengali hate speech</td>\n      <td>টিনের চালে কাওয়া, তুমি আমার শাওয়া</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>জাজদের কে কে ঘন্টা হিসেবে কানে গলায় থাপরাতে চা...</td>\n      <td>0</td>\n      <td>entertainment</td>\n      <td>Bengali hate speech</td>\n      <td>জাজদের কে কে ঘন্টা হিসেবে কানে গলায় থাপরাতে চা...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>বেয়াদব</td>\n      <td>0</td>\n      <td>Meme, TikTok and others</td>\n      <td>Bengali hate speech</td>\n      <td>বেয়াদব</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3995</th>\n      <td>দেওয়ানবাগী হচ্ছে এক্সট্রিম লেভেল এর মাদারচোদ।।</td>\n      <td>1</td>\n      <td>religion</td>\n      <td>Bengali hate speech</td>\n      <td>দেওয়ানবাগী হচ্ছে এক্সট্রিম লেভেল এর মাদারচোদ।।</td>\n    </tr>\n    <tr>\n      <th>3996</th>\n      <td>মাদারচদদের কেউ ব্যবহার শেখা।গাজা খাইয়া জাজ হইছ...</td>\n      <td>1</td>\n      <td>entertainment</td>\n      <td>Bengali hate speech</td>\n      <td>মাদারচদদের কেউ ব্যবহার শেখা।গাজা খাইয়া জাজ হইছ...</td>\n    </tr>\n    <tr>\n      <th>3997</th>\n      <td>এই মাদারচোদেরা কি তদন্তের কথা বলে</td>\n      <td>1</td>\n      <td>crime</td>\n      <td>Bengali hate speech</td>\n      <td>এই মাদারচোদেরা কি তদন্তের কথা বলে</td>\n    </tr>\n    <tr>\n      <th>3998</th>\n      <td>ওসি মোয়াজ্জিম যদি রেকডিং না করলে আমরা নুসরাতের...</td>\n      <td>0</td>\n      <td>crime</td>\n      <td>Bengali hate speech</td>\n      <td>ওসি মোয়াজ্জিম যদি রেকডিং না করলে আমরা নুসরাতের...</td>\n    </tr>\n    <tr>\n      <th>3999</th>\n      <td>আল্লাহ বাচাইচে কুন দাড়ি ওয়ালা নাই একটা দাড়িওয়া...</td>\n      <td>0</td>\n      <td>crime</td>\n      <td>Bengali hate speech</td>\n      <td>আল্লাহ বাচাইচে কুন দাড়ি ওয়ালা নাই একটা দাড়িওয়া...</td>\n    </tr>\n  </tbody>\n</table>\n<p>4000 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"## Custom Dataset Class\n- This class can be used for all Sentiment Analysis fine-tuning and zero shot tasks\n- For multiple languages, combine data into one dataframe as the dataset class takes DATAFRAME as input","metadata":{}},{"cell_type":"code","source":"class MultilingualSentimentAnalysis_Dataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length=256):\n        self.tokenizer = tokenizer\n        self.dataframe = dataframe\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        label = self.dataframe.iloc[idx][\"Labels Set\"]\n        input_text = self.dataframe.iloc[idx][\"Processed_Post\"]\n        if pd.isna(input_text):\n            input_text = \"\"\n\n        # Tokenize\n        encoding = self.tokenizer.encode_plus(\n            input_text, None,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'  # Corrected the argument here\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'token_type_ids': encoding['token_type_ids'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.float)\n        }\n","metadata":{"execution":{"iopub.status.busy":"2024-11-17T16:06:22.700867Z","iopub.execute_input":"2024-11-17T16:06:22.701147Z","iopub.status.idle":"2024-11-17T16:06:22.709758Z","shell.execute_reply.started":"2024-11-17T16:06:22.701116Z","shell.execute_reply":"2024-11-17T16:06:22.708867Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\n# Get the model name\n\nmodel_name = \"ai4bharat/indic-bert\"\n\n\n# Initialize IndicBERT tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-11-17T16:06:22.710861Z","iopub.execute_input":"2024-11-17T16:06:22.711140Z","iopub.status.idle":"2024-11-17T16:06:25.703314Z","shell.execute_reply.started":"2024-11-17T16:06:22.711110Z","shell.execute_reply":"2024-11-17T16:06:25.702473Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Initialise dataset instances\ntrain_dataset = MultilingualSentimentAnalysis_Dataset(train_df, tokenizer)\ndev_dataset = MultilingualSentimentAnalysis_Dataset(test_df, tokenizer)\n\n# Initialise the dataloaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ndev_dataloader = DataLoader(dev_dataset, batch_size=32, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-17T16:06:25.704496Z","iopub.execute_input":"2024-11-17T16:06:25.704840Z","iopub.status.idle":"2024-11-17T16:06:25.711031Z","shell.execute_reply.started":"2024-11-17T16:06:25.704806Z","shell.execute_reply":"2024-11-17T16:06:25.710013Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import torch\n\nimport torch.nn as nn\n\n\n\nclass MBertForSentimentAnalysis(nn.Module):\n\n    def __init__(self, freeze_bert=False):\n\n        super(MBertForSentimentAnalysis, self).__init__()\n\n\n\n        # Load mBERT model and tokenizer\n\n        self.model_name = \"ai4bharat/indic-bert\"\n\n        # tokenizer = BertTokenizer.from_pretrained(model_name)\n\n        self.mbert = AutoModel.from_pretrained(self.model_name)\n\n\n\n        # Add a batch normalization layer\n\n        self.batch_norm = nn.BatchNorm1d(self.mbert.config.hidden_size)\n\n        \n\n        # Add a linear layer for classification\n\n        self.classification = nn.Linear(self.mbert.config.hidden_size, 2)\n\n\n\n        # Option to freeze MBERT layers to prevent them from being updated during training\n\n        if freeze_bert:\n\n            for param in self.mbert.parameters():\n\n                param.requires_grad = False\n\n\n\n    def forward(self, input_ids, attention_mask):\n\n        # Get the output from BERT model\n\n        _, pooled_outputs = self.mbert(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n\n\n\n        # Pass output through batch normalization layer\n\n        pooled_outputs = self.batch_norm(pooled_outputs)\n\n        \n\n        # Pass output through linear layer\n\n        out = self.classification(pooled_outputs)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-11-17T16:06:25.712283Z","iopub.execute_input":"2024-11-17T16:06:25.712582Z","iopub.status.idle":"2024-11-17T16:06:25.727556Z","shell.execute_reply.started":"2024-11-17T16:06:25.712551Z","shell.execute_reply":"2024-11-17T16:06:25.726627Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"- Pass freeze as true when doing zero shot to prevent BERT from getting fine-tuned","metadata":{}},{"cell_type":"code","source":"# Freeze BERT encoder to prevent fine-tuning\nmodel = MBertForSentimentAnalysis()\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    model.to(device)  # Move model to CUDA device if available\n    print(\"Using CUDA\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"CUDA is not available. Using CPU instead.\")","metadata":{"execution":{"iopub.status.busy":"2024-11-17T16:06:25.729802Z","iopub.execute_input":"2024-11-17T16:06:25.730125Z","iopub.status.idle":"2024-11-17T16:06:25.952024Z","shell.execute_reply.started":"2024-11-17T16:06:25.730094Z","shell.execute_reply":"2024-11-17T16:06:25.951025Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using CUDA\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\nmse_loss = torch.nn.CrossEntropyLoss()\n\nnum_epochs = 4\n\ntrain_losses = []\nval_losses = []\n\nfor epoch in range(num_epochs):\n    # Training\n    model.train()  \n    train_loss = 0\n    for batch in train_dataloader:\n        # Forward pass\n        inputs = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n        loss = mse_loss(outputs, inputs['labels'].long())\n\n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        train_loss += loss.item()\n\n    # Validation\n    model.eval()  \n    val_loss = 0\n    with torch.no_grad():  # No need to compute gradients during validation\n        for batch in dev_dataloader:\n            inputs = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n            loss = mse_loss(outputs, inputs['labels'].long())\n            val_loss += loss.item()\n\n    # Calculate average losses\n    avg_train_loss = train_loss / len(train_dataloader)\n    avg_val_loss = val_loss / len(dev_dataloader)\n\n    # Append the losses for plotting\n    train_losses.append(avg_train_loss)\n    val_losses.append(avg_val_loss)\n\n    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-11-17T16:06:25.953037Z","iopub.execute_input":"2024-11-17T16:06:25.953319Z","iopub.status.idle":"2024-11-17T16:17:12.048866Z","shell.execute_reply.started":"2024-11-17T16:06:25.953289Z","shell.execute_reply":"2024-11-17T16:17:12.047829Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1/4, Train Loss: 0.6125, Validation Loss: 0.6947\nEpoch 2/4, Train Loss: 0.4379, Validation Loss: 0.4601\nEpoch 3/4, Train Loss: 0.3263, Validation Loss: 0.5988\nEpoch 4/4, Train Loss: 0.2252, Validation Loss: 0.4455\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"### Evaluation of Fine-Tuned Model\n- Evaluate fine-tuned model on test dataset of each language","metadata":{}},{"cell_type":"code","source":"# Load datasets for each language\nlanguages = [\"Hindi\", \"Marathi\", \"Bengali\", \"Tamil\", \"Telugu\"]\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\n\nwith torch.no_grad():\n    for lang in languages:\n        test_file = f\"/kaggle/working/CSE556-NLP-Project/Hate-Speech-Detection-Experiments/Dataset/{lang}_test.csv\"\n        test_df = pd.read_csv(test_file)\n        test_dataloader = DataLoader(MultilingualSentimentAnalysis_Dataset(test_df, tokenizer), batch_size=16, shuffle=True)\n        \n        # Make list for predicted labels and ground truth labels\n        predicted_labels = []\n        labels = []\n\n        # Perform inference\n        for batch in test_dataloader:\n            inputs = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n            predicted_labels.extend(torch.argmax(outputs, dim=1).tolist())\n            labels.extend(inputs['labels'].long().tolist())\n        \n\n        # Print results for a particular language\n        print(f\"RESULTS FOR {lang}\")\n        print()\n        # Calculate accuracy\n        accuracy = accuracy_score(labels, predicted_labels) \n        print(f'Accuracy: {accuracy:.4f}')\n\n        # Calculate F1-score\n        weighted_f1_score = f1_score(labels, predicted_labels, average='weighted')\n        macro_f1_score = f1_score(labels, predicted_labels, average='macro')\n        print(f'Weighted F1-score: {weighted_f1_score:.4f}')\n        print(f'Macro F1-score: {macro_f1_score:.4f}')\n        print()\n\n        # Print classification report\n        print(\"Classification Report\")\n        print(classification_report(labels, predicted_labels))\n        print()\n        # Print confusion matrix\n        print(\"Confusion Matrix\")\n        print(confusion_matrix(labels, predicted_labels))\n        print()\n        print(\"----------*******************---------------\")\n    ","metadata":{"execution":{"iopub.status.busy":"2024-11-17T16:17:12.050800Z","iopub.execute_input":"2024-11-17T16:17:12.051245Z","iopub.status.idle":"2024-11-17T16:18:18.441801Z","shell.execute_reply.started":"2024-11-17T16:17:12.051195Z","shell.execute_reply":"2024-11-17T16:18:18.440809Z"},"trusted":true},"outputs":[{"name":"stdout","text":"RESULTS FOR Hindi\n\nAccuracy: 0.5060\nWeighted F1-score: 0.4014\nMacro F1-score: 0.4014\n\nClassification Report\n              precision    recall  f1-score   support\n\n           0       0.50      0.92      0.65       500\n           1       0.54      0.09      0.15       500\n\n    accuracy                           0.51      1000\n   macro avg       0.52      0.51      0.40      1000\nweighted avg       0.52      0.51      0.40      1000\n\n\nConfusion Matrix\n[[462  38]\n [456  44]]\n\n----------*******************---------------\nRESULTS FOR Marathi\n\nAccuracy: 0.5480\nWeighted F1-score: 0.4986\nMacro F1-score: 0.4986\n\nClassification Report\n              precision    recall  f1-score   support\n\n           0       0.53      0.86      0.66       500\n           1       0.63      0.23      0.34       500\n\n    accuracy                           0.55      1000\n   macro avg       0.58      0.55      0.50      1000\nweighted avg       0.58      0.55      0.50      1000\n\n\nConfusion Matrix\n[[431  69]\n [383 117]]\n\n----------*******************---------------\nRESULTS FOR Bengali\n\nAccuracy: 0.8210\nWeighted F1-score: 0.8210\nMacro F1-score: 0.8210\n\nClassification Report\n              precision    recall  f1-score   support\n\n           0       0.82      0.82      0.82       500\n           1       0.82      0.82      0.82       500\n\n    accuracy                           0.82      1000\n   macro avg       0.82      0.82      0.82      1000\nweighted avg       0.82      0.82      0.82      1000\n\n\nConfusion Matrix\n[[412  88]\n [ 91 409]]\n\n----------*******************---------------\nRESULTS FOR Tamil\n\nAccuracy: 0.6390\nWeighted F1-score: 0.5616\nMacro F1-score: 0.4816\n\nClassification Report\n              precision    recall  f1-score   support\n\n           0       0.65      0.93      0.77       640\n           1       0.49      0.12      0.20       360\n\n    accuracy                           0.64      1000\n   macro avg       0.57      0.53      0.48      1000\nweighted avg       0.60      0.64      0.56      1000\n\n\nConfusion Matrix\n[[595  45]\n [316  44]]\n\n----------*******************---------------\nRESULTS FOR Telugu\n\nAccuracy: 0.5210\nWeighted F1-score: 0.4554\nMacro F1-score: 0.4554\n\nClassification Report\n              precision    recall  f1-score   support\n\n           0       0.51      0.87      0.64       500\n           1       0.57      0.17      0.27       500\n\n    accuracy                           0.52      1000\n   macro avg       0.54      0.52      0.46      1000\nweighted avg       0.54      0.52      0.46      1000\n\n\nConfusion Matrix\n[[434  66]\n [413  87]]\n\n----------*******************---------------\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Save Model and Tokenizer\nmodel_save_path = \"IndicBert_HS_Bn_FineTune.pth\"\ntorch.save(model.state_dict(), model_save_path)","metadata":{"execution":{"iopub.status.busy":"2024-11-17T16:18:18.443320Z","iopub.execute_input":"2024-11-17T16:18:18.443676Z","iopub.status.idle":"2024-11-17T16:18:18.620780Z","shell.execute_reply.started":"2024-11-17T16:18:18.443609Z","shell.execute_reply":"2024-11-17T16:18:18.619927Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}